#!/usr/bin/env python3
"""
–¢–µ—Å—Ç SQL-agent —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º JSON —Ñ–∞–π–ª–æ–≤ –∏–∑ datasets
–í–∫–ª—é—á–∞–µ—Ç –≤—Å–µ —Å—Ö–µ–º—ã –¥–∞–Ω–Ω—ã—Ö: flights, questsH, linear_schema, star_schema, network_schema
"""

import asyncio
import time
import sys
import os
import json
import glob
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor
import statistics

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—é
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from sql_agent.task_manager import SimpleTaskManager
from sql_agent.models import OptimizationRequest


class TestResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ–¥–Ω–æ–π –∑–∞–¥–∞—á–∏"""
    def __init__(self, task_id: str, dataset_name: str, success: bool,
                 execution_time: float, error: str = None, result_data: Dict = None):
        self.task_id = task_id
        self.dataset_name = dataset_name
        self.success = success
        self.execution_time = execution_time
        self.error = error
        self.result_data = result_data or {}
        self.quality_score = None


class TestRunner:
    """–ö–ª–∞—Å—Å –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Ç–µ—Å—Ç–æ–≤"""

    def __init__(self, max_concurrent_tasks: int = 5):
        self.max_concurrent_tasks = max_concurrent_tasks
        self.task_manager = SimpleTaskManager(
            max_workers=max_concurrent_tasks,
            task_timeout_minutes=10
        )
        self.test_results: List[TestResult] = []

    def load_test_datasets(self) -> List[Dict[str, Any]]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ JSON —Ñ–∞–π–ª–æ–≤"""
        datasets = []
        datasets_dir = os.path.join(os.path.dirname(__file__), "datasets")

        if not os.path.exists(datasets_dir):
            print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {datasets_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return datasets

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        priority_files = [
            "flights.json",
            "questsH.json",
            "linear_schema.json",
            "star_schema.json",
            "network_schema.json"
        ]

        # –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Ñ–∞–π–ª—ã
        for filename in priority_files:
            json_file = os.path.join(datasets_dir, filename)
            if os.path.exists(json_file):
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        dataset_name = filename.replace('.json', '')
                        datasets.append({
                            'name': dataset_name,
                            'data': data,
                            'priority': True
                        })
                        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {dataset_name}")
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {filename}: {e}")

        # –ó–∞—Ç–µ–º –∑–∞–≥—Ä—É–∂–∞–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã
        json_files = glob.glob(os.path.join(datasets_dir, "*.json"))
        for json_file in json_files:
            filename = os.path.basename(json_file)
            if filename not in priority_files:
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        dataset_name = filename.replace('.json', '')
                        datasets.append({
                            'name': dataset_name,
                            'data': data,
                            'priority': False
                        })
                        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {dataset_name}")
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {filename}: {e}")

        return datasets

    async def run_single_test(self, dataset: Dict[str, Any]) -> TestResult:
        """–ó–∞–ø—É—Å–∫ –æ–¥–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞"""
        dataset_name = dataset['name']
        data = dataset['data']

        print(f"üöÄ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞ –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞: {dataset_name}")
        print(f"   - DDL –∫–æ–º–∞–Ω–¥: {len(data.get('ddl', []))}")
        print(f"   - –ó–∞–ø—Ä–æ—Å–æ–≤: {len(data.get('queries', []))}")

        try:
            # –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å –∏–∑ –¥–∞–Ω–Ω—ã—Ö JSON
            request = OptimizationRequest(
                url=data['url'],
                ddl=data['ddl'],
                queries=data['queries']
            )

            start_time = time.time()

            # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É
            task_id = self.task_manager.create_task(request)

            # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–º–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è–º–∏
            step = 0
            while self.task_manager.get_task_status(task_id) == "RUNNING":
                await asyncio.sleep(2)
                step += 2
                if step % 30 == 0:  # –ö–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥
                    print(f"   ‚è≥ {dataset_name} –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —É–∂–µ {step} —Å–µ–∫—É–Ω–¥...")

            execution_time = time.time() - start_time
            status = self.task_manager.get_task_status(task_id)

            if status == "DONE":
                result = self.task_manager.get_task_result(task_id)
                result_data = {
                    "ddl_count": len(result.ddl),
                    "migrations_count": len(result.migrations),
                    "queries_count": len(result.queries),
                    "quality_score": result.quality_score
                }
                print(f"‚úÖ –¢–µ—Å—Ç {dataset_name} –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ –∑–∞ {execution_time:.2f}s")
                print(f"   - DDL –∫–æ–º–∞–Ω–¥: {len(result.ddl)}")
                print(f"   - –ú–∏–≥—Ä–∞—Ü–∏–π: {len(result.migrations)}")
                print(f"   - –ó–∞–ø—Ä–æ—Å–æ–≤: {len(result.queries)}")
                print(f"   - –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {result.quality_score}")
                return TestResult(task_id, dataset_name, True, execution_time, None, result_data)
            else:
                error = self.task_manager.get_task_error(task_id)
                print(f"‚ùå –¢–µ—Å—Ç {dataset_name} –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π: {error}")
                return TestResult(task_id, dataset_name, False, execution_time, error)

        except Exception as e:
            execution_time = time.time() - start_time if 'start_time' in locals() else 0
            error_msg = f"–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —Ç–µ—Å—Ç–∞: {str(e)}"
            print(f"‚ùå –¢–µ—Å—Ç {dataset_name} –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º: {error_msg}")
            return TestResult("", dataset_name, False, execution_time, error_msg)

    async def run_concurrent_tests(self, datasets: List[Dict[str, Any]]) -> List[TestResult]:
        """–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á"""
        print(f"üîÑ –ó–∞–ø—É—Å–∫ {len(datasets)} —Ç–µ—Å—Ç–æ–≤ —Å –º–∞–∫—Å–∏–º—É–º {self.max_concurrent_tasks} –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏")

        # –°–æ–∑–¥–∞–µ–º —Å–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á
        semaphore = asyncio.Semaphore(self.max_concurrent_tasks)

        async def run_with_semaphore(dataset):
            async with semaphore:
                return await self.run_single_test(dataset)

        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ —Ç–µ—Å—Ç—ã
        tasks = [run_with_semaphore(dataset) for dataset in datasets]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        test_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                dataset_name = datasets[i]['name']
                error_msg = f"–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —Ç–µ—Å—Ç–∞: {str(result)}"
                print(f"‚ùå –¢–µ—Å—Ç {dataset_name} –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º: {error_msg}")
                test_results.append(TestResult("", dataset_name, False, 0, error_msg))
            else:
                test_results.append(result)

        return test_results

    def print_statistics(self, results: List[TestResult]):
        """–í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ —Ç–µ—Å—Ç–∞–º"""
        if not results:
            print("üìä –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏")
            return

        total_tests = len(results)
        successful_tests = sum(1 for r in results if r.success)
        failed_tests = total_tests - successful_tests

        execution_times = [r.execution_time for r in results if r.success]

        print("\n" + "="*60)
        print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
        print("="*60)
        print(f"–í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: {total_tests}")
        print(f"–£—Å–ø–µ—à–Ω—ã—Ö: {successful_tests} ({successful_tests/total_tests*100:.1f}%)")
        print(f"–ù–µ—É–¥–∞—á–Ω—ã—Ö: {failed_tests} ({failed_tests/total_tests*100:.1f}%)")

        if execution_times:
            print(f"\n‚è±Ô∏è  –í–†–ï–ú–Ø –í–´–ü–û–õ–ù–ï–ù–ò–Ø:")
            print(f"   –°—Ä–µ–¥–Ω–µ–µ: {statistics.mean(execution_times):.2f}s")
            print(f"   –ú–µ–¥–∏–∞–Ω–∞: {statistics.median(execution_times):.2f}s")
            print(f"   –ú–∏–Ω–∏–º—É–º: {min(execution_times):.2f}s")
            print(f"   –ú–∞–∫—Å–∏–º—É–º: {max(execution_times):.2f}s")

        # –ê–Ω–∞–ª–∏–∑ –ø–æ —Ç–∏–ø–∞–º —Å—Ö–µ–º
        print(f"\nüèóÔ∏è  –ê–ù–ê–õ–ò–ó –ü–û –¢–ò–ü–ê–ú –°–•–ï–ú:")
        schema_types = {
            "flights": "–ê–≤–∏–∞–ø–µ—Ä–µ–ª–µ—Ç—ã (1 —Ç–∞–±–ª–∏—Ü–∞, 20 –∑–∞–ø—Ä–æ—Å–æ–≤)",
            "questsH": "–ö–≤–µ—Å—Ç-–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ (37 —Ç–∞–±–ª–∏—Ü, 10 –∑–∞–ø—Ä–æ—Å–æ–≤)",
            "linear_schema": "–õ–∏–Ω–µ–π–Ω–∞—è —Å—Ö–µ–º–∞ (4 —Ç–∞–±–ª–∏—Ü—ã, 3 –∑–∞–ø—Ä–æ—Å–∞)",
            "star_schema": "–ó–≤–µ–∑–¥–æ–æ–±—Ä–∞–∑–Ω–∞—è —Å—Ö–µ–º–∞ (5 —Ç–∞–±–ª–∏—Ü, 4 –∑–∞–ø—Ä–æ—Å–∞)",
            "network_schema": "–°–µ—Ç–µ–≤–∞—è —Å—Ö–µ–º–∞ (5 —Ç–∞–±–ª–∏—Ü, 5 –∑–∞–ø—Ä–æ—Å–æ–≤)"
        }

        for result in results:
            schema_type = result.dataset_name
            type_description = schema_types.get(schema_type, "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø")
            status = "‚úÖ –£—Å–ø–µ—Ö" if result.success else "‚ùå –û—à–∏–±–∫–∞"
            quality_info = f" (–æ—Ü–µ–Ω–∫–∞: {result.result_data.get('quality_score', 'N/A')})" if result.success else ""
            print(f"   {schema_type.upper()}: {status}{quality_info} - {type_description}")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        quality_scores = [r.result_data.get('quality_score') for r in results if r.success and r.result_data.get('quality_score')]
        if quality_scores:
            print(f"\nüéØ –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ö–ê–ß–ï–°–¢–í–ê:")
            print(f"   –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: {statistics.mean(quality_scores):.1f}/100")
            print(f"   –ú–µ–¥–∏–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: {statistics.median(quality_scores):.1f}/100")
            print(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: {min(quality_scores):.1f}/100")
            print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: {max(quality_scores):.1f}/100")

        print(f"\nüìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ú–ï–ù–ï–î–ñ–ï–†–ê –ó–ê–î–ê–ß:")
        stats = self.task_manager.get_stats()
        for key, value in stats.items():
            print(f"   {key}: {value}")

        if failed_tests > 0:
            print(f"\n‚ùå –ù–ï–£–î–ê–ß–ù–´–ï –¢–ï–°–¢–´:")
            for result in results:
                if not result.success:
                    print(f"   - {result.dataset_name}: {result.error}")

        print("="*60)


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("üß™ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è SQL-agent —Å JSON –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏")
    print("="*80)
    print("–¢–µ—Å—Ç–∏—Ä—É–µ–º –≤—Å–µ —Å—Ö–µ–º—ã –¥–∞–Ω–Ω—ã—Ö: flights, questsH, linear, star, network")

    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π —Ä–∞–Ω–Ω–µ—Ä
    test_runner = TestRunner(max_concurrent_tasks=5)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    print("üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    datasets = test_runner.load_test_datasets()

    if not datasets:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ç–µ—Å—Ç–æ–≤")
        return

    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(datasets)} –¥–∞—Ç–∞—Å–µ—Ç–æ–≤")

    # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç—ã
    start_time = time.time()
    results = await test_runner.run_concurrent_tests(datasets)
    total_time = time.time() - start_time

    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    test_runner.print_statistics(results)

    print(f"\nüéâ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {total_time:.2f}s")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results_data = []
    for result in results:
        results_data.append({
            "dataset_name": result.dataset_name,
            "success": result.success,
            "execution_time": result.execution_time,
            "error": result.error,
            "result_data": result.result_data
        })

    with open("test_results.json", "w", encoding="utf-8") as f:
        json.dump(results_data, f, indent=2, ensure_ascii=False)

    print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ test_results.json")


if __name__ == "__main__":
    asyncio.run(main())
