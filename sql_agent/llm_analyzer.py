"""
LLMAnalyzer ‚Äî –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ SQL —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM –∏ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
1. LLM –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ö–µ–º—É –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
2. –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–≤–∏–∂–æ–∫ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–∞–ª–∏–¥–Ω—ã–π SQL —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏
3. –†–µ–∞–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ —á–µ—Ä–µ–∑ sqlglot (–∑–∞–º–µ–Ω–∞ SELECT *, –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ LIMIT)
4. –†–æ–±–∞—Å—Ç–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å–ª–æ–∂–Ω—ã—Ö SQL —Å—Ç—Ä—É–∫—Ç—É—Ä

–ö–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
- –ü–æ–ª–Ω–æ—Å—Ç—å—é –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥: LLM –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞, —à–∞–±–ª–æ–Ω—ã –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
- –†–µ–∞–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ SQL —á–µ—Ä–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–µ AST, –Ω–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
- –†–æ–±–∞—Å—Ç–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ sqlglot —Å fallback –Ω–∞ —É–ª—É—á—à–µ–Ω–Ω—ã–π regex
- –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö
"""

import json
import logging
import os
import re
from typing import Dict, Any, Optional, Tuple, List
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

from dotenv import load_dotenv
from openai import OpenAI

try:
    import sqlglot
    SQLGLOT_AVAILABLE = True
except ImportError:
    SQLGLOT_AVAILABLE = False
    logging.warning("sqlglot –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. SQL-–ø–∞—Ä—Å–∏–Ω–≥ –±—É–¥–µ—Ç –æ—Ç–∫–ª—é—á–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install sqlglot")

load_dotenv()
logger = logging.getLogger(__name__)


# ---------- –ò—Å–∫–ª—é—á–µ–Ω–∏—è ----------
class LLMAnalyzerError(RuntimeError):
    """–§–∞—Ç–∞–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ LLM —Å –ø–æ–ª–Ω—ã–º –æ–ø–∏—Å–∞–Ω–∏–µ–º –∏—Å—Ö–æ–¥–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –∏ –ø—Ä–∏—á–∏–Ω."""

    def __init__(self, message: str, details: Optional[Dict[str, Any]] = None):
        super().__init__(message)
        self.details = details or {}


# ---------- –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä ----------
class LLMAnalyzer:
    """–ì–∏–±—Ä–∏–¥–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ë–î: LLM –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ + –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è SQL."""

    # ---------- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ----------
    def __init__(
            self,
            api_key: Optional[str] = None,
            base_url: Optional[str] = None,
            max_workers: int = 6,
    ):
        self.max_workers = max_workers
        self.api_key = api_key or os.getenv("OPEN_ROUTER")
        self.base_url = base_url or "https://openrouter.ai/api/v1"

        if not self.api_key:
            raise ValueError("API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ OPEN_ROUTER –≤ .env —Ñ–∞–π–ª–µ")

        self.client = OpenAI(base_url=self.base_url, api_key=self.api_key)

        self.analysis_model = "nvidia/nemotron-nano-9b-v2"
        self.evaluation_model = "google/gemini-2.5-flash-preview-09-2025"

        # –ü–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å
        self._errors_lock = Lock()
        self._results_lock = Lock()

        # SQL-–ø–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ sqlglot
        self.enable_sql_parsing = SQLGLOT_AVAILABLE
        if not self.enable_sql_parsing:
            logger.warning("SQL-–ø–∞—Ä—Å–∏–Ω–≥ –æ—Ç–∫–ª—é—á–µ–Ω. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –±—É–¥—É—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã.")

        self.provider = "openrouter"
        self.model = self.analysis_model

        logger.info("LLM Analyzer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:")
        logger.info(f"  - Provider: {self.provider}")
        logger.info(f"  - Analysis model: {self.analysis_model}")
        logger.info(f"  - SQL parsing: {'–≤–∫–ª—é—á–µ–Ω' if self.enable_sql_parsing else '–æ—Ç–∫–ª—é—á–µ–Ω'}")

    # ---------- –ü—É–±–ª–∏—á–Ω—ã–π –º–µ—Ç–æ–¥ ----------
    def analyze_database(self, request_data: Dict[str, Any]) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ë–î —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–æ–π."""
        try:
            self._validate_input(request_data)

            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –∏–∑ URL
            catalog_name = self._extract_catalog_from_url(request_data.get("url", ""))
            original_ddl = request_data.get("ddl", [])
            
            # –ï—Å–ª–∏ –∫–∞—Ç–∞–ª–æ–≥ = default_catalog, –ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –∏–∑ DDL
            if catalog_name == "default_catalog" and original_ddl:
                catalog_from_ddl = self._extract_catalog_from_ddl(original_ddl)
                if catalog_from_ddl and catalog_from_ddl != "public":
                    catalog_name = catalog_from_ddl
                    logger.info(f"‚úÖ –ö–∞—Ç–∞–ª–æ–≥ –∏–∑–≤–ª–µ—á–µ–Ω –∏–∑ DDL: {catalog_name}")

            logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞—Ç–∞–ª–æ–≥: {catalog_name}")

            # ====== –ü–û–î–ö–õ–Æ–ß–ï–ù–ò–ï –ö –ë–î –î–õ–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ò ======
            from .db_connector import DatabaseConnector

            db_connector = None
            table_stats = {}

            try:
                db_connector = DatabaseConnector(request_data.get("url", ""))
                if db_connector.connect():
                    logger.info("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ, –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É...")

                    # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–∞–∂–¥–æ–π —Ç–∞–±–ª–∏—Ü–µ
                    stats_collected = 0
                    for ddl_item in original_ddl:
                        statement = ddl_item.get("statement", "")
                        table_name = self._extract_table_name_robust(statement)
                        if table_name:
                            stats = db_connector.get_table_stats(table_name)
                            column_stats = db_connector.get_column_stats(table_name)

                            # –°—á–∏—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ —Ö–æ—Ç—è –±—ã —á—Ç–æ-—Ç–æ
                            if stats or column_stats:
                                table_stats[table_name] = {
                                    "row_count": stats.get("row_count", 0),
                                    "size_bytes": stats.get("total_size_bytes", 0),
                                    "column_stats": column_stats
                                }
                                stats_collected += 1

                    if stats_collected > 0:
                        logger.info(f"üìä –ü–æ–ª—É—á–µ–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è {stats_collected}/{len(original_ddl)} —Ç–∞–±–ª–∏—Ü")
                    else:
                        logger.info(f"‚ÑπÔ∏è –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ë–î –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ (–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–∞–≤), –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å—Ö–µ–º—ã")
                else:
                    logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –ë–î, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ë–î: {e}")
                logger.info("‚ÑπÔ∏è –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ö–µ–º—ã")
            finally:
                if db_connector:
                    db_connector.close()

            # ====== –®–ê–ì 1: LLM –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–∞–±–ª–∏—Ü—ã ======
            logger.info("=" * 70)
            logger.info("–®–ê–ì 1/4: LLM –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ö–µ–º—É –ë–î")
            logger.info("=" * 70)

            tables_analysis = self._analyze_ddl_with_llm(original_ddl, catalog_name, table_stats)

            # ====== –®–ê–ì 2: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º DDL –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ ======
            logger.info("=" * 70)
            logger.info("–®–ê–ì 2/4: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è DDL (–¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è)")
            logger.info("=" * 70)

            new_ddl = self._generate_ddl_deterministic(tables_analysis, catalog_name)

            # ====== –®–ê–ì 3: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –º–∏–≥—Ä–∞—Ü–∏–∏ ======
            logger.info("=" * 70)
            logger.info("–®–ê–ì 3/4: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–∏–≥—Ä–∞—Ü–∏–π")
            logger.info("=" * 70)

            mig_payload = {
                "url": request_data.get("url"),
                "catalog_name": catalog_name,  # –Ø–≤–Ω–æ –ø–µ—Ä–µ–¥–∞–µ–º –∫–∞—Ç–∞–ª–æ–≥
                "old_ddl": original_ddl,
                "new_ddl": new_ddl,
            }
            mig_input_raw = json.dumps(mig_payload, ensure_ascii=False, indent=2)

            mig_obj = self._call_with_retries(
                function_name="produce_migrations",
                function_schema=self._tool_schema_migrations(),
                system_prompt=self._system_prompt_migrations(),
                user_input=mig_input_raw,
            )
            migrations = mig_obj["migrations"]

            # ====== –®–ê–ì 4: –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å—ã ======
            logger.info("=" * 70)
            logger.info("–®–ê–ì 4/4: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ)")
            logger.info("=" * 70)

            optimized_queries = self._optimize_queries_parallel(
                request_data.get("queries", []),
                new_ddl,
                catalog_name
            )

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = {
                "ddl": new_ddl,
                "migrations": migrations,
                "queries": optimized_queries,
                "_meta": {
                    "llm_provider": self.provider,
                    "llm_model": self.model,
                    "mode": "hybrid",
                    "sql_parsing_enabled": self.enable_sql_parsing,
                    "had_errors": False,
                    "errors": [],
                    "warnings": [],
                },
            }

            self._validate_full_paths(result, catalog_name)

            # ‚úÖ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê
            logger.info("=" * 70)
            logger.info("–û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê –†–ï–ó–£–õ–¨–¢–ê–¢–ê")
            logger.info("=" * 70)

            evaluation_score, evaluation_details = self._evaluate_result_internal(
                request_data,
                result
            )

            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ü–µ–Ω–∫—É –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            result["_meta"]["quality_score"] = evaluation_score
            result["_meta"]["quality_details"] = evaluation_details

            logger.info("=" * 70)
            logger.info("‚úÖ –ê–Ω–∞–ª–∏–∑ –ë–î —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω")
            logger.info(f"üìä –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {evaluation_score}/100")
            logger.info("=" * 70)

            return result

        except LLMAnalyzerError:
            raise
        except Exception as e:
            logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)}", exc_info=True)
            raise LLMAnalyzerError(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)}", {"original_error": str(e)})

    # ========================================================================
    # –ì–ò–ë–†–ò–î–ù–´–ô –ü–û–î–•–û–î: LLM –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç ‚Üí –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–≤–∏–∂–æ–∫ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç
    # ========================================================================
    def _evaluate_result_internal(
            self,
            request_data: Dict[str, Any],
            result: Dict[str, Any]
    ) -> Tuple[int, Dict[str, Any]]:
        """–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—Ü–µ–Ω–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º."""
        try:
            task_input = json.dumps({
                "url": request_data.get("url"),
                "ddl_count": len(request_data.get("ddl", [])),
                "queries_count": len(request_data.get("queries", []))
            }, indent=2)

            output = json.dumps({
                "ddl_count": len(result.get("ddl", [])),
                "migrations_count": len(result.get("migrations", [])),
                "queries_count": len(result.get("queries", [])),
                "mode": result["_meta"]["mode"]
            }, indent=2)

            prompt = f"""Evaluate database optimization result on 100-point scale.

    Input:
    {task_input}

    Output:
    {output}

    Criteria:
    1. DDL Quality (25): Structure, optimizations, full paths
    2. Migration Quality (25): Strategy, validation
    3. Query Optimization (25): Performance, correctness
    4. Execution Time (15): Expected gains
    5. Storage (10): Compression, efficiency

    Return JSON:
    {{
      "score": <1-100>,
      "ddl_score": <0-25>,
      "migration_score": <0-25>,
      "query_score": <0-25>,
      "execution_score": <0-15>,
      "storage_score": <0-10>,
      "strengths": ["strength1", "strength2"],
      "weaknesses": ["weakness1", "weakness2"],
      "recommendations": ["rec1", "rec2"]
    }}"""

            logger.info("üì§ –ó–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏ –æ—Ü–µ–Ω–∫–∏...")

            response = self.client.chat.completions.create(
                model=self.evaluation_model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
                max_tokens=1500,
                extra_headers={
                    "HTTP-Referer": "http://localhost",
                    "X-Title": "SQL Agent Quality Evaluation"
                }
            )

            content = response.choices[0].message.content.strip()

            json_content = self._extract_json_from_response(content)
            if not json_content:
                logger.warning("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –æ—Ü–µ–Ω–∫–∏ –Ω–µ –≤–µ—Ä–Ω—É–ª–∞ JSON, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –æ—Ü–µ–Ω–∫—É")
                return 70, {"note": "Evaluation model failed, using default score"}

            eval_result = json.loads(json_content)
            score = eval_result.get("score", 70)

            if not (1 <= score <= 100):
                logger.warning(f"‚ö†Ô∏è –û—Ü–µ–Ω–∫–∞ –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞: {score}, –∏—Å–ø–æ–ª—å–∑—É–µ–º 70")
                score = 70

            logger.info("üì• –†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ü–µ–Ω–∫–∏:")
            logger.info(f"   üéØ –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞: {score}/100")
            logger.info(f"   üìã DDL –∫–∞—á–µ—Å—Ç–≤–æ: {eval_result.get('ddl_score', 'N/A')}/25")
            logger.info(f"   üîÑ –ú–∏–≥—Ä–∞—Ü–∏–∏: {eval_result.get('migration_score', 'N/A')}/25")
            logger.info(f"   ‚ö° –ó–∞–ø—Ä–æ—Å—ã: {eval_result.get('query_score', 'N/A')}/25")
            logger.info(f"   ‚è±Ô∏è  –í—Ä–µ–º—è: {eval_result.get('execution_score', 'N/A')}/15")
            logger.info(f"   üíæ –•—Ä–∞–Ω–µ–Ω–∏–µ: {eval_result.get('storage_score', 'N/A')}/10")

            strengths = eval_result.get("strengths", [])
            if strengths:
                logger.info("   ‚úÖ –°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã:")
                for s in strengths:
                    logger.info(f"      - {s}")

            weaknesses = eval_result.get("weaknesses", [])
            if weaknesses:
                logger.info("   ‚ö†Ô∏è  –°–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã:")
                for w in weaknesses:
                    logger.info(f"      - {w}")

            recommendations = eval_result.get("recommendations", [])
            if recommendations:
                logger.info("   üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
                for r in recommendations:
                    logger.info(f"      - {r}")

            details = {
                "ddl_score": eval_result.get("ddl_score"),
                "migration_score": eval_result.get("migration_score"),
                "query_score": eval_result.get("query_score"),
                "execution_score": eval_result.get("execution_score"),
                "storage_score": eval_result.get("storage_score"),
                "strengths": strengths,
                "weaknesses": weaknesses,
                "recommendations": recommendations
            }

            return score, details

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}")
            return 70, {"error": str(e), "note": "Evaluation failed, using default score"}

    def _analyze_ddl_with_llm(
            self,
            original_ddl: List[Dict[str, Any]],
            catalog_name: str,
            table_stats: Dict[str, Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """LLM –¢–û–õ–¨–ö–û –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–∞–±–ª–∏—Ü—ã –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏."""

        if table_stats is None:
            table_stats = {}

        tables_analysis = []

        for ddl_item in original_ddl:
            statement = ddl_item.get("statement", "")
            if "CREATE TABLE" not in statement.upper():
                continue

            table_name = self._extract_table_name_robust(statement)
            if not table_name:
                continue

            columns = self._extract_columns_robust(statement)
            if not columns:
                continue

            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–∞–±–ª–∏—Ü—ã
            stats = table_stats.get(table_name, {})
            row_count = stats.get("row_count", "unknown")
            size_bytes = stats.get("size_bytes", 0)
            column_stats = stats.get("column_stats", {})

            # LLM –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¢–û–õ–¨–ö–û —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Å —É—á–µ—Ç–æ–º —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            columns_data = [{
                "name": c[0],
                "type": c[1],
                "distinct_count": column_stats.get(c[0], {}).get("distinct_count", "unknown"),
                "cardinality": column_stats.get(c[0], {}).get("cardinality", "unknown")
            } for c in columns]

            analysis_prompt = f"""Analyze this table structure and suggest optimization strategy.

Table: {table_name}
Row count: {row_count}
Size: {size_bytes} bytes
Columns: {json.dumps(columns_data, indent=2)}

Return JSON with optimization strategy:
{{
  "table_name": "{table_name}",
  "partition_columns": ["column_name"],
  "cluster_columns": ["column_name"],
  "compression": "ZSTD",
  "rationale": "brief explanation based on data statistics"
}}

Rules:
- Max 2 partition columns (prefer high-cardinality date/timestamp)
- Max 4 cluster columns (prefer columns used in JOIN/WHERE with good cardinality)
- Consider row_count and data distribution
- Return ONLY valid JSON"""

            try:
                resp = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": analysis_prompt}],
                    temperature=0.1,
                    max_tokens=500,
                )

                content = (resp.choices[0].message.content or "").strip()
                json_content = self._extract_json_from_response(content)

                if json_content:
                    analysis = json.loads(json_content)
                    tables_analysis.append({
                        "table_name": table_name,
                        "original_columns": columns,
                        "partition_columns": analysis.get("partition_columns", [])[:2],
                        "cluster_columns": analysis.get("cluster_columns", [])[:4],
                        "compression": analysis.get("compression", "ZSTD"),
                    })
                else:
                    tables_analysis.append(self._heuristic_analysis(
                        table_name,
                        columns,
                        row_count=row_count if isinstance(row_count, int) else 0,
                        column_stats=column_stats
                    ))

            except Exception as e:
                logger.warning(f"LLM analysis failed for {table_name}: {e}")
                logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è {table_name}")
                tables_analysis.append(self._heuristic_analysis(
                    table_name,
                    columns,
                    row_count=row_count if isinstance(row_count, int) else 0,
                    column_stats=column_stats
                ))

        return tables_analysis

    def _heuristic_analysis(
            self,
            table_name: str,
            columns: List[Tuple[str, str]],
            row_count: int = 0,
            column_stats: Dict[str, Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """–≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å —É—á–µ—Ç–æ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö."""

        if column_stats is None:
            column_stats = {}

        partition_cols = []
        cluster_cols = []

        for col_name, col_type in columns:
            col_lower = col_name.lower()
            type_upper = col_type.upper()

            col_stat = column_stats.get(col_name, {})
            cardinality = col_stat.get("cardinality", 0)

            # –ü–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ: –¥–∞—Ç–∞ + high cardinality
            if any(kw in col_lower for kw in ['date', 'time', 'created', 'updated']):
                if any(t in type_upper for t in ['DATE', 'TIMESTAMP', 'TIME']):
                    if cardinality > 10 or cardinality == 0:
                        partition_cols.append(col_name)

            # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è: ID, key + —Å—Ä–µ–¥–Ω—è—è cardinality
            if any(kw in col_lower for kw in ['id', 'key', 'code', 'type', 'status']):
                if row_count == 0 or (10 < cardinality < row_count * 0.5) or cardinality == 0:
                    cluster_cols.append(col_name)

        return {
            "table_name": table_name,
            "original_columns": columns,
            "partition_columns": partition_cols[:2],
            "cluster_columns": cluster_cols[:4],
            "compression": "ZSTD",
        }

    def _generate_ddl_deterministic(
            self,
            tables_analysis: List[Dict[str, Any]],
            catalog_name: str
    ) -> List[Dict[str, str]]:
        """–î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è DDL –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ LLM."""

        ddl_statements = []

        ddl_statements.append({
            "statement": f"CREATE SCHEMA IF NOT EXISTS {catalog_name}.optimized"
        })

        for analysis in tables_analysis:
            table_name = analysis["table_name"]
            columns = analysis["original_columns"]
            partition_cols = analysis["partition_columns"]
            cluster_cols = analysis["cluster_columns"]
            compression = analysis["compression"]

            columns_sql = ",\n  ".join([f"{col[0]} {col[1]}" for col in columns])

            with_parts = [
                "format = 'ICEBERG'",
                f"'write.compression-codec' = '{compression}'",
                "'write.target-file-size-bytes' = '268435456'",
                "'read.vectorization.enabled' = 'true'",
                "'write.parquet.compression-codec' = 'ZSTD'",
                "'write.parquet.page-size-bytes' = '1048576'",
                "'write.parquet.row-group-size-bytes' = '134217728'",
            ]

            if partition_cols:
                valid_partitions = [c for c in partition_cols if any(col[0] == c for col in columns)]
                if valid_partitions:
                    parts_str = ", ".join([f"'{c}'" for c in valid_partitions])
                    with_parts.insert(1, f"partitioning = ARRAY[{parts_str}]")

            if cluster_cols:
                valid_clusters = [c for c in cluster_cols if any(col[0] == c for col in columns)]
                if valid_clusters:
                    clusters_str = ", ".join([f"'{c}'" for c in valid_clusters])
                    insert_pos = 2 if partition_cols else 1
                    with_parts.insert(insert_pos, f"clustering = ARRAY[{clusters_str}]")

            with_clause = ",\n  ".join(with_parts)

            ddl = f"""CREATE TABLE {catalog_name}.optimized.{table_name} (
  {columns_sql}
) WITH (
  {with_clause}
)"""

            ddl_statements.append({"statement": ddl})
            logger.info(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω DDL –¥–ª—è {table_name}")

        return ddl_statements

    # ========================================================================
    # –†–û–ë–ê–°–¢–ù–´–ô SQL-–ü–ê–†–°–ò–ù–ì —á–µ—Ä–µ–∑ sqlglot
    # ========================================================================

    def _extract_table_name_robust(self, ddl: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º—è —Ç–∞–±–ª–∏—Ü—ã —Å —É—á–µ—Ç–æ–º –∫–∞–≤—ã—á–µ–∫ –∏ —Å–ª–æ–∂–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤."""
        if self.enable_sql_parsing:
            try:
                parsed = sqlglot.parse_one(ddl, dialect="trino")
                table = parsed.find(sqlglot.exp.Table)
                if table:
                    return table.name
            except Exception as e:
                logger.debug(f"sqlglot parse failed, fallback to regex: {e}")

        match = re.search(
            r'CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?'
            r'(?:(?:`|")?(\w+)(?:`|")?\.)?'
            r'(?:(?:`|")?(\w+)(?:`|")?\.)?'
            r'(?:`|")?(\w+)(?:`|")?',
            ddl,
            re.IGNORECASE
        )
        if match:
            return match.group(3) or match.group(2) or match.group(1)
        return None

    def _extract_columns_robust(self, ddl: str) -> List[Tuple[str, str]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ —Å —É—á–µ—Ç–æ–º –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∏ —Å–ª–æ–∂–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä."""
        if self.enable_sql_parsing:
            try:
                parsed = sqlglot.parse_one(ddl, dialect="trino")
                columns = []

                # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫
                try:
                    for col_def in parsed.find_all(sqlglot.exp.ColumnDef):
                        col_name = col_def.this.name
                        col_type = col_def.kind.sql(dialect="trino") if col_def.kind else "UNKNOWN"
                        columns.append((col_name, col_type))
                except (AttributeError, TypeError) as inner_e:
                    logger.debug(f"Cannot extract columns via sqlglot: {inner_e}")
                    raise  # –ü–µ—Ä–µ–¥–∞–µ–º –≤–æ –≤–Ω–µ—à–Ω–∏–π except

                if columns:
                    return columns
            except Exception as e:
                logger.debug(f"sqlglot column extraction failed, fallback to regex: {e}")

        match = re.search(r'\(([^)]+(?:\([^)]*\)[^)]*)*)\)', ddl, re.DOTALL | re.IGNORECASE)
        if not match:
            return []

        columns_text = match.group(1)
        columns = []
        current_col = []
        depth = 0

        for char in columns_text + ',':
            if char in '([<':
                depth += 1
                current_col.append(char)
            elif char in ')]>':
                depth -= 1
                current_col.append(char)
            elif char == ',' and depth == 0:
                col_str = ''.join(current_col).strip()
                if col_str and not col_str.upper().startswith(('PRIMARY', 'FOREIGN', 'CONSTRAINT', 'CHECK')):
                    parts = col_str.split(None, 1)
                    if len(parts) >= 2:
                        col_name = parts[0].strip('`"[]')
                        col_type = parts[1].split()[0]
                        columns.append((col_name, col_type))
                current_col = []
            else:
                current_col.append(char)

        return columns

    # ========================================================================
    # –†–ï–ê–õ–¨–ù–´–ï –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò —á–µ—Ä–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–µ SQL, –Ω–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
    # ========================================================================

    def _optimize_queries_parallel(
            self,
            queries: List[Dict[str, Any]],
            new_ddl: List[Dict[str, str]],
            catalog_name: str
    ) -> List[Dict[str, Any]]:
        """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–µ—Ç –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å—ã."""

        table_metadata = self._extract_table_metadata(new_ddl)

        results = []
        errors = []

        def _optimize_one_query(q: Dict[str, Any]) -> Dict[str, Any]:
            try:
                queryid = q.get("queryid")
                original_query = q.get("query", "")

                query_with_new_paths = self._replace_table_paths_robust(
                    original_query,
                    catalog_name
                )

                optimized_query = self._apply_real_optimizations(
                    query_with_new_paths,
                    table_metadata,
                    catalog_name
                )

                # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –¢–æ–ª—å–∫–æ queryid –∏ query —Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó
                return {
                    "queryid": queryid,
                    "query": optimized_query
                }

            except Exception as e:
                logger.error(f"Query optimization failed for {q.get('queryid')}: {e}")
                with self._errors_lock:
                    errors.append({
                        "queryid": q.get("queryid"),
                        "error": str(e),
                        "traceback": str(e)
                    })
                raise

        with ThreadPoolExecutor(max_workers=self.max_workers) as ex:
            futures = {ex.submit(_optimize_one_query, q): q for q in queries}

            for fut in as_completed(futures):
                try:
                    result = fut.result()
                    with self._results_lock:
                        results.append(result)
                except Exception:
                    pass

        if errors:
            raise LLMAnalyzerError(
                f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å {len(errors)} –∏–∑ {len(queries)} –∑–∞–ø—Ä–æ—Å–æ–≤",
                {"optimization_errors": errors}
            )

        return results

    def _extract_table_metadata(self, ddl_list: List[Dict[str, str]]) -> Dict[str, Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π."""
        metadata = {}

        for ddl_item in ddl_list:
            statement = ddl_item.get("statement", "")
            if "CREATE TABLE" not in statement.upper():
                continue

            table_name = self._extract_table_name_robust(statement)
            if not table_name:
                continue

            columns = self._extract_columns_robust(statement)
            partition_cols = self._extract_array_from_with(statement, "partitioning")
            cluster_cols = self._extract_array_from_with(statement, "clustering")

            metadata[table_name] = {
                "columns": [c[0] for c in columns],
                "partition_columns": partition_cols,
                "cluster_columns": cluster_cols,
            }

        return metadata

    def _extract_array_from_with(self, ddl: str, property_name: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–∞—Å—Å–∏–≤ –∏–∑ WITH clause."""
        pattern = rf"{property_name}\s*=\s*ARRAY\[(.*?)\]"
        match = re.search(pattern, ddl, re.IGNORECASE)
        if match:
            items = match.group(1).split(',')
            return [item.strip().strip("'\"") for item in items]
        return []

    def _clean_sql_for_parsing(self, sql: str) -> str:
        """
        –û—á–∏—Å—Ç–∫–∞ SQL –æ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –ø–µ—Ä–µ–¥ –ø–∞—Ä—Å–∏–Ω–≥–æ–º sqlglot.
        
        –£–¥–∞–ª—è–µ—Ç –æ–¥–Ω–æ—Å—Ç—Ä–æ—á–Ω—ã–µ (--) –∏ –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–µ (/* */) –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏,
        –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –≤—ã–∑–≤–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ.
        """
        # –£–¥–∞–ª—è–µ–º –æ–¥–Ω–æ—Å—Ç—Ä–æ—á–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ --
        sql = re.sub(r'--[^\n]*', '', sql)
        
        # –£–¥–∞–ª—è–µ–º –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ /* */
        sql = re.sub(r'/\*.*?\*/', '', sql, flags=re.DOTALL)
        
        return sql.strip()

    def _apply_real_optimizations(
            self,
            query: str,
            table_metadata: Dict[str, Dict[str, Any]],
            catalog_name: str
    ) -> str:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –†–ï–ê–õ–¨–ù–´–ï –∏–∑–º–µ–Ω–µ–Ω–∏—è –∫ SQL, –Ω–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏."""

        optimized = query
        applied = []

        if not self.enable_sql_parsing:
            return self._apply_simple_optimizations(query)

        try:
            # –û—á–∏—â–∞–µ–º SQL –æ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –ø–µ—Ä–µ–¥ –ø–∞—Ä—Å–∏–Ω–≥–æ–º
            clean_query = self._clean_sql_for_parsing(optimized)
            parsed = sqlglot.parse_one(clean_query, dialect="trino")

            modified_star = self._replace_select_star_sqlglot(parsed, table_metadata, catalog_name)
            if modified_star:
                applied.append("column pruning (SELECT *)")

            if not self._has_limit(parsed) and not self._is_aggregation_sqlglot(parsed):
                parsed = self._add_limit_sqlglot(parsed, 10000)
                applied.append("automatic LIMIT 10000")

            partition_used = self._check_partition_usage(parsed, table_metadata)
            if partition_used:
                applied.extend([f"partition pruning on {t}.{c}" for t, c in partition_used])

            cluster_joins = self._check_cluster_joins(parsed, table_metadata)
            if cluster_joins:
                applied.extend([f"clustered join on {t}.{c}" for t, c in cluster_joins])

            optimized = parsed.sql(dialect="trino")

            if applied:
                logger.info(f"‚úÖ –†–µ–∞–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {', '.join(applied)}")

            return optimized

        except Exception as e:
            logger.warning(f"SQL optimization failed, using simple approach: {e}")
            return self._apply_simple_optimizations(query)

    def _replace_select_star_sqlglot(
            self,
            parsed: sqlglot.Expression,
            table_metadata: Dict[str, Dict[str, Any]],
            catalog_name: str
    ) -> bool:
        """–ó–∞–º–µ–Ω—è–µ—Ç SELECT * –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏."""
        modified = False

        try:
            for select in parsed.find_all(sqlglot.exp.Select):
                if len(select.expressions) == 1:
                    expr = select.expressions[0]
                    if isinstance(expr, sqlglot.exp.Star):
                        from_clause = select.find(sqlglot.exp.From)
                        if from_clause and from_clause.this:
                            table_name = from_clause.this.name

                            if table_name in table_metadata:
                                columns = table_metadata[table_name]["columns"]
                                if columns:
                                    select.expressions = [
                                        sqlglot.exp.Column(this=col)
                                        for col in columns
                                    ]
                                    modified = True
                                    logger.info(f"‚úÖ –ó–∞–º–µ–Ω–µ–Ω SELECT * –Ω–∞ {len(columns)} –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è {table_name}")
        except (AttributeError, TypeError) as e:
            logger.debug(f"Cannot replace SELECT *: {e}")

        return modified

    def _has_limit(self, parsed: sqlglot.Expression) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ LIMIT."""
        return parsed.find(sqlglot.exp.Limit) is not None

    def _add_limit_sqlglot(self, parsed: sqlglot.Expression, limit: int) -> sqlglot.Expression:
        """–î–æ–±–∞–≤–ª—è–µ—Ç LIMIT –∫ –∑–∞–ø—Ä–æ—Å—É."""
        return parsed.limit(limit)

    def _is_aggregation_sqlglot(self, parsed: sqlglot.Expression) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –∞–≥—Ä–µ–≥–∞—Ç–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –∏–ª–∏ GROUP BY."""
        try:
            has_group = parsed.find(sqlglot.exp.Group) is not None
            # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∞–≥—Ä–µ–≥–∞—Ç–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π
            agg_funcs = list(parsed.find_all(sqlglot.exp.AggFunc))
            has_agg = len(agg_funcs) > 0
            return has_group or has_agg
        except (AttributeError, TypeError) as e:
            logger.debug(f"Cannot check aggregation: {e}, assuming no aggregation")
            return False

    def _check_partition_usage(
            self,
            parsed: sqlglot.Expression,
            table_metadata: Dict[str, Dict[str, Any]]
    ) -> List[Tuple[str, str]]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –≤ WHERE."""
        used = []

        try:
            where = parsed.find(sqlglot.exp.Where)
            if not where:
                return used

            where_columns = {col.name for col in where.find_all(sqlglot.exp.Column)}

            for table, meta in table_metadata.items():
                for part_col in meta["partition_columns"]:
                    if part_col in where_columns:
                        used.append((table, part_col))
        except (AttributeError, TypeError) as e:
            logger.debug(f"Cannot check partition usage: {e}")

        return used

    def _check_cluster_joins(
            self,
            parsed: sqlglot.Expression,
            table_metadata: Dict[str, Dict[str, Any]]
    ) -> List[Tuple[str, str]]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç JOIN –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–º –∫–æ–ª–æ–Ω–∫–∞–º."""
        used = []

        try:
            for join in parsed.find_all(sqlglot.exp.Join):
                if not join.on:
                    continue

                join_columns = {col.name for col in join.on.find_all(sqlglot.exp.Column)}

                for table, meta in table_metadata.items():
                    for cluster_col in meta["cluster_columns"]:
                        if cluster_col in join_columns:
                            used.append((table, cluster_col))
        except (AttributeError, TypeError) as e:
            logger.debug(f"Cannot check cluster joins: {e}")

        return used

    def _apply_simple_optimizations(self, query: str) -> str:
        """–ü—Ä–æ—Å—Ç—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –±–µ–∑ sqlglot."""
        optimized = query

        if "LIMIT" not in optimized.upper() and not self._is_aggregation_query(optimized):
            optimized = optimized.rstrip(';') + "\nLIMIT 10000;"

        return optimized

    def _is_aggregation_query(self, query: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å –∞–≥—Ä–µ–≥–∞—Ü–∏–æ–Ω–Ω—ã–º."""
        upper_query = query.upper()
        return any(kw in upper_query for kw in ["GROUP BY", "COUNT(", "SUM(", "AVG(", "MAX(", "MIN(", "HAVING"])

    def _replace_table_paths_robust(self, query: str, catalog_name: str) -> str:
        """–ó–∞–º–µ–Ω—è–µ—Ç –ø—É—Ç–∏ —Ç–∞–±–ª–∏—Ü —á–µ—Ä–µ–∑ sqlglot –∏–ª–∏ regex."""
        if self.enable_sql_parsing:
            try:
                parsed = sqlglot.parse_one(query, dialect="trino")

                # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –æ–±—Ö–æ–¥ —Ç–∞–±–ª–∏—Ü
                try:
                    for table in parsed.find_all(sqlglot.exp.Table):
                        if table.catalog and table.db:
                            if table.db.lower() != "optimized":
                                table.set("db", sqlglot.exp.Identifier(this="optimized"))
                                table.set("catalog", sqlglot.exp.Identifier(this=catalog_name))
                except (AttributeError, TypeError) as inner_e:
                    logger.debug(f"Cannot iterate tables: {inner_e}, fallback to regex")
                    raise  # –ü–µ—Ä–µ–¥–∞–µ–º –≤–æ –≤–Ω–µ—à–Ω–∏–π except

                return parsed.sql(dialect="trino")
            except Exception as e:
                logger.debug(f"sqlglot path replacement failed, fallback to regex: {e}")

        pattern = r'(\w+)\.(\w+)\.(\w+)'

        def replace_path(match):
            cat, schema, table = match.groups()
            if schema.lower() != 'optimized':
                return f"{catalog_name}.optimized.{table}"
            return match.group(0)

        return re.sub(pattern, replace_path, query)

    def _call_with_retries(
            self,
            function_name: str,
            function_schema: Dict[str, Any],
            system_prompt: str,
            user_input: str,
            max_attempts: int = 3
    ) -> Dict[str, Any]:
        """
        –í—ã–∑–æ–≤ LLM —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ –∏ repair.
        
        Args:
            max_attempts: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 3)
        """
        all_errors = []
        
        for attempt in range(1, max_attempts + 1):
            # –î–ª—è –ø–µ—Ä–≤–æ–π –ø–æ–ø—ã—Ç–∫–∏ repair_prompt = None
            # –î–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö - –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—à–∏–±–∫–∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –ø–æ–ø—ã—Ç–∫–∏
            repair_prompt = None
            if attempt > 1:
                prev_errors = all_errors[-1]["errors"]
                prev_raw = all_errors[-1].get("raw_output", "")
                repair_prompt = self._build_repair_prompt(prev_errors, prev_raw, function_name=function_name)
                logger.warning(f"{function_name}: –ø–æ–ø—ã—Ç–∫–∞ {attempt}/{max_attempts} (—Å repair)")
            
            ok, obj, raw, errs = self._call_llm_function(
                function_name=function_name,
                function_schema=function_schema,
                system_prompt=system_prompt,
                user_input=user_input,
                repair_prompt=repair_prompt,
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–ø—ã—Ç–∫–∏
            all_errors.append({
                "attempt": attempt,
                "errors": errs,
                "raw_output": self._safe_truncate(raw or "", 4000)
            })
            
            if ok:
                if attempt > 1:
                    logger.info(f"‚úÖ {function_name}: —É—Å–ø–µ—à–Ω–æ –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt}/{max_attempts}")
                return obj

        # –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å
        error_details = {
            "function": function_name,
            "model": self.analysis_model,
            "attempts": all_errors,
        }

        raise LLMAnalyzerError(
            f"–ú–æ–¥–µ–ª—å –Ω–µ –≤–µ—Ä–Ω—É–ª–∞ –≤–∞–ª–∏–¥–Ω—ã–π JSON –¥–ª—è '{function_name}' –ø–æ—Å–ª–µ {max_attempts} –ø–æ–ø—ã—Ç–æ–∫",
            error_details,
        )

    def _call_llm_function(
            self,
            function_name: str,
            function_schema: Dict[str, Any],
            system_prompt: str,
            user_input: str,
            repair_prompt: Optional[str],
    ) -> Tuple[bool, Optional[Dict[str, Any]], Optional[str], List[Dict[str, Any]]]:
        """–í—ã–∑–æ–≤ LLM."""
        errors: List[Dict[str, Any]] = []
        try:
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_input},
            ]
            if repair_prompt:
                messages.append({"role": "system", "content": repair_prompt})

            kwargs: Dict[str, Any] = dict(
                model=self.model,
                messages=messages,
                temperature=0.1,
                max_tokens=12000,
            )
            if self.provider == "openrouter":
                kwargs["extra_headers"] = {
                    "HTTP-Referer": "http://localhost",
                    "X-Title": f"SQL Agent {function_name}",
                }

            resp = self.client.chat.completions.create(**kwargs)
            choice = resp.choices[0]
            content = (choice.message.content or "").strip()

            if not content:
                errors.append({"type": "empty_llm_response", "message": "Model returned no content"})
                return False, None, None, errors

            json_content = self._extract_json_from_response(content)
            if not json_content:
                errors.append({"type": "json_not_found", "message": "No valid JSON found"})
                return False, None, content, errors

            try:
                parsed = json.loads(json_content)
            except json.JSONDecodeError as e:
                errors.append({"type": "json_decode_error", "message": str(e)})
                return False, None, json_content, errors

            ok, msg = self._validate_by_function(function_name, parsed)
            if not ok:
                errors.append({"type": "schema_validation", "message": msg})
                return False, None, json_content, errors

            return True, parsed, json_content, errors

        except Exception as e:
            errors.append({"type": "llm_request_error", "message": str(e)})
            return False, None, None, errors

    def _extract_json_from_response(self, content: str) -> Optional[str]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ—á–∏—Å—Ç–∫–æ–π.
        
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –æ—Ç–≤–µ—Ç–æ–≤ LLM:
        - –ß–∏—Å—Ç—ã–π JSON
        - JSON –≤ markdown –±–ª–æ–∫–∞—Ö ```json
        - JSON —Å —Ç–µ–∫—Å—Ç–æ–º –¥–æ/–ø–æ—Å–ª–µ
        - JSON —Å trailing commas
        """
        # –£–¥–∞–ª—è–µ–º markdown –±–ª–æ–∫–∏
        cleaned = re.sub(r'```json\s*', '', content)
        cleaned = re.sub(r'```javascript\s*', '', cleaned)
        cleaned = re.sub(r'\s*```\s*', '', cleaned)
        cleaned = re.sub(r'```\s*', '', cleaned).strip()
        
        # –£–¥–∞–ª—è–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–π —Ç–µ–∫—Å—Ç –ø–µ—Ä–µ–¥ JSON
        cleaned = re.sub(r'^[^{]*', '', cleaned)

        start = cleaned.find('{')
        if start == -1:
            return None

        depth = 0
        for i, char in enumerate(cleaned[start:], start):
            if char == '{':
                depth += 1
            elif char == '}':
                depth -= 1
                if depth == 0:
                    try:
                        json_str = cleaned[start:i + 1]
                        
                        # –ü—ã—Ç–∞–µ–º—Å—è —É–±—Ä–∞—Ç—å trailing commas –ø–µ—Ä–µ–¥ –ø–∞—Ä—Å–∏–Ω–≥–æ–º
                        json_str = re.sub(r',(\s*[}\]])', r'\1', json_str)
                        
                        json.loads(json_str)
                        return json_str
                    except json.JSONDecodeError:
                        pass

        return None

    @staticmethod
    def _tool_schema_migrations() -> Dict[str, Any]:
        return {
            "type": "function",
            "function": {
                "name": "produce_migrations",
                "description": "Return ONLY the migration statements to move data from old DDL to new DDL.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "migrations": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {"statement": {"type": "string"}},
                                "required": ["statement"],
                            },
                        }
                    },
                    "required": ["migrations"],
                    "additionalProperties": False,
                },
            },
        }

    def _validate_by_function(self, fn: str, data: Dict[str, Any]) -> Tuple[bool, str]:
        if fn == "produce_migrations":
            if not isinstance(data, dict) or "migrations" not in data or not isinstance(data["migrations"], list):
                return False, "produce_migrations: missing or invalid 'migrations' array"
            for i, item in enumerate(data["migrations"]):
                if not isinstance(item, dict) or "statement" not in item or not isinstance(item["statement"], str):
                    return False, f"produce_migrations.migrations[{i}] must be {{statement: string}}"
            return True, "ok"

        return False, f"unknown function '{fn}'"

    @staticmethod
    def _system_prompt_migrations() -> str:
        """–ü—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–∏–≥—Ä–∞—Ü–∏–π."""
        return (
            "You are an expert data migration specialist for LARGE-SCALE database optimization.\n"
            "Your task: Create comprehensive migration statements to transfer data from old DDL to new optimized structure.\n"
            "\n"
            "CRITICAL REQUIREMENTS:\n"
            "1. USE the 'catalog_name' field from input payload - do NOT extract from URL\n"
            "2. MANDATORY: ALL paths must use format: <catalog_name>.optimized.<table_name>\n"
            "3. Extract table names from old_ddl and new_ddl\n"
            "4. Migrate ALL columns from original tables to optimized tables\n"
            "5. Include SELECT COUNT(*) validation queries for each migration\n"
            "\n"
            "PATH EXAMPLES:\n"
            "- If catalog_name='flights': flights.optimized.flights_table\n"
            "- If catalog_name='analytics': analytics.optimized.users\n"
            "- If old table was 'flights.public.flights', new is 'flights.optimized.flights'\n"
            "\n"
            "MANDATORY JSON OUTPUT FORMAT:\n"
            "{\n"
            '  "migrations": [\n'
            '    {"statement": "INSERT INTO <catalog_name>.optimized.<table> SELECT * FROM <catalog_name>.public.<table>"},\n'
            '    {"statement": "SELECT COUNT(*) as validation FROM <catalog_name>.optimized.<table>"}\n'
            '  ]\n'
            "}\n"
            "\n"
            "CRITICAL: Replace <catalog_name> with the actual catalog_name from input!\n"
            "Return ONLY valid JSON, no markdown, no explanations."
        )

    @staticmethod
    def _build_repair_prompt(errors: List[Dict[str, Any]], raw_output: Optional[str], function_name: str) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏."""
        reasons = []
        has_json_error = False
        has_schema_error = False
        
        for e in errors or []:
            t = e.get("type", "unknown")
            msg = e.get("message", "")
            reasons.append(f"- {t}: {msg}")
            
            if t in ["json_decode_error", "json_not_found"]:
                has_json_error = True
            elif t == "schema_validation":
                has_schema_error = True
        
        reasons_text = "\n".join(reasons) if reasons else "- unknown failure"

        snippet = (raw_output or "")[:800]
        
        # –ë–∞–∑–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
        prompt = (
            f"REPAIR REQUIRED: Your previous JSON output was invalid.\n"
            "Fix the issues and return ONLY valid JSON.\n"
            "\n"
            "Issues detected:\n"
            f"{reasons_text}\n"
            "\n"
        )
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –æ—à–∏–±–∫–∏
        if has_json_error:
            prompt += (
                "CRITICAL JSON SYNTAX ERRORS DETECTED:\n"
                "- Remove ALL trailing commas before } or ]\n"
                "- Use double quotes (\") for strings, NOT single quotes\n"
                "- Do NOT include any text, markdown, or code blocks\n"
                "- Do NOT add comments inside JSON\n"
                "- Start with { and end with }\n"
                "\n"
            )
        
        if has_schema_error:
            prompt += (
                "SCHEMA VALIDATION ERRORS DETECTED:\n"
                f"- Ensure all required fields for '{function_name}' are present\n"
                "- Use full paths: <catalog>.optimized.<table>\n"
                "- Check the 'catalog_name' field in input and use it in ALL paths\n"
                "- Each statement must have 'statement' field\n"
                "- Each query must have 'queryid' and 'query' fields\n"
                "\n"
                "PATH FORMAT REMINDER:\n"
                "- DDL: CREATE TABLE <catalog_name>.optimized.<table_name>\n"
                "- Migration: INSERT INTO <catalog_name>.optimized.<table> SELECT * FROM <catalog_name>.<old_schema>.<table>\n"
                "- Query: FROM <catalog_name>.optimized.<table>\n"
                "\n"
            )
        
        prompt += (
            f"Previous invalid output (first 800 chars):\n{snippet}\n"
            "\n"
            "FINAL INSTRUCTION: Return ONLY valid JSON, no markdown, no explanations, no extra text."
        )
        
        return prompt

    @staticmethod
    def _validate_input(data: Dict[str, Any]) -> None:
        if "ddl" not in data or not isinstance(data["ddl"], list):
            raise LLMAnalyzerError("–í—Ö–æ–¥–Ω–æ–π –æ–±—ä–µ–∫—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –º–∞—Å—Å–∏–≤ 'ddl'")
        if "queries" not in data or not isinstance(data["queries"], list):
            raise LLMAnalyzerError("–í—Ö–æ–¥–Ω–æ–π –æ–±—ä–µ–∫—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –º–∞—Å—Å–∏–≤ 'queries'")

    @staticmethod
    def _validate_full_paths(result: Dict[str, Any], catalog_name: str) -> None:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—É—Ç–µ–π —Å —É—á–µ—Ç–æ–º —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –∫–∞—Ç–∞–ª–æ–≥–æ–≤.
        
        –ü—Ä–∏–Ω–∏–º–∞–µ—Ç:
        - catalog.optimized.table (—Å—Ç—Ä–æ–≥–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ)
        - any_catalog.optimized.table (–ª—é–±–æ–π –∫–∞—Ç–∞–ª–æ–≥ —Å optimized)
        - CREATE SCHEMA statements (–ø—Ä–æ–ø—É—Å–∫–∞–µ–º)
        """
        # –ì–∏–±–∫–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω: –ª—é–±–æ–π –∫–∞—Ç–∞–ª–æ–≥ + .optimized. + –∏–º—è —Ç–∞–±–ª–∏—Ü—ã
        optimized_pattern = r'\w+\.optimized\.\w+'
        
        def has_optimized_path(text: str) -> bool:
            """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –ø—É—Ç–∏ —Å .optimized."""
            return bool(re.search(optimized_pattern, text))
        
        def is_schema_statement(text: str) -> bool:
            """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —è–≤–ª—è–µ—Ç—Å—è –ª–∏ statement —Å–æ–∑–¥–∞–Ω–∏–µ–º —Å—Ö–µ–º—ã."""
            return bool(re.search(r'CREATE\s+SCHEMA', text, re.IGNORECASE))

        errors = []
        warnings = []

        for i, ddl_item in enumerate(result.get("ddl", [])):
            statement = ddl_item.get("statement", "")
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º CREATE SCHEMA
            if is_schema_statement(statement):
                continue
                
            if "CREATE TABLE" in statement.upper():
                if not has_optimized_path(statement):
                    errors.append(f"DDL[{i}] missing .optimized. path: {statement[:150]}...")

        for i, mig_item in enumerate(result.get("migrations", [])):
            statement = mig_item.get("statement", "")
            
            # SELECT –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–∂–Ω–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å
            if statement.upper().strip().startswith("SELECT"):
                continue
                
            if "INSERT INTO" in statement.upper():
                if not has_optimized_path(statement):
                    errors.append(f"Migration[{i}] missing .optimized. path: {statement[:150]}...")

        for i, query_item in enumerate(result.get("queries", [])):
            query = query_item.get("query", "")
            if "FROM" in query.upper() or "JOIN" in query.upper():
                if not has_optimized_path(query):
                    # –î–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–∞–µ–º warning, –∞ –Ω–µ error (–º–æ–≥—É—Ç –±—ã—Ç—å –ø–æ–¥–∑–∞–ø—Ä–æ—Å—ã –±–µ–∑ –ø—É—Ç–µ–π)
                    warnings.append(f"Query[{i}] may be missing .optimized. path")

        if errors:
            raise LLMAnalyzerError(
                f"–í–∞–ª–∏–¥–∞—Ü–∏—è –ø—É—Ç–µ–π –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∞: {len(errors)} –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫",
                {
                    "validation_errors": errors,
                    "warnings": warnings,
                    "catalog_name": catalog_name,
                    "hint": "DDL –∏ –º–∏–≥—Ä–∞—Ü–∏–∏ –î–û–õ–ñ–ù–´ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—É—Ç–∏ –≤–∏–¥–∞: <catalog>.optimized.<table>"
                }
            )

    @staticmethod
    def _safe_truncate(text: str, limit: int) -> str:
        return text if len(text) <= limit else text[:limit] + "... [truncated]"

    def _extract_catalog_from_ddl(self, ddl_list: List[Dict[str, str]]) -> Optional[str]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–∏ –∫–∞—Ç–∞–ª–æ–≥–∞ –∏–∑ DDL statements.
        
        –ò—â–µ—Ç 3-—á–∞—Å—Ç–Ω—ã–µ –ø—É—Ç–∏ –≤–∏–¥–∞ catalog.schema.table –≤ CREATE TABLE statements.
        """
        try:
            for ddl_item in ddl_list[:5]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 5 DDL
                statement = ddl_item.get("statement", "")
                
                # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω: CREATE TABLE catalog.schema.table
                match = re.search(
                    r'CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?(\w+)\.(\w+)\.(\w+)',
                    statement,
                    re.IGNORECASE
                )
                
                if match:
                    catalog = match.group(1)
                    schema = match.group(2)
                    table = match.group(3)
                    
                    # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å—Ö–µ–º—ã
                    if schema.lower() not in ['information_schema', 'pg_catalog', 'sys']:
                        logger.debug(f"–ù–∞–π–¥–µ–Ω –ø—É—Ç—å –≤ DDL: {catalog}.{schema}.{table}")
                        return catalog
            
            return None
        except Exception as e:
            logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–∞—Ç–∞–ª–æ–≥ –∏–∑ DDL: {e}")
            return None

    def _extract_catalog_from_url(self, url: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–∏ –∫–∞—Ç–∞–ª–æ–≥–∞ –∏–∑ JDBC URL"""
        try:
            trino_match = re.search(r'catalog=([^&]+)', url)
            if trino_match:
                catalog = trino_match.group(1)
                logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω –∫–∞—Ç–∞–ª–æ–≥ –∏–∑ Trino URL: {catalog}")
                return catalog

            if 'jdbc://' in url:
                url_part = url.replace('jdbc://', '')
                if '/' in url_part:
                    db_part = url_part.split('/')[-1]
                    if '?' in db_part:
                        db_name = db_part.split('?')[0]
                    else:
                        db_name = db_part
                    if db_name:
                        logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω –∫–∞—Ç–∞–ª–æ–≥ –∏–∑ JDBC URL: {db_name}")
                        return db_name

            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–∞—Ç–∞–ª–æ–≥ –∏–∑ URL: {url}")
            return "default_catalog"
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–∞—Ç–∞–ª–æ–≥–∞ –∏–∑ URL {url}: {e}")
            return "default_catalog"

