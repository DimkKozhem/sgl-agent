"""
LLMAnalyzer ‚Äî —Ç—Ä—ë—Ö—à–∞–≥–æ–≤—ã–π –ø–∞–π–ø–ª–∞–π–Ω —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏ –∏ –∂—ë—Å—Ç–∫–∏–º JSON-–∫–æ–Ω—Ç—Ä–æ–ª–µ–º.

–í—Ö–æ–¥ (user_input_raw):
{
  "url": "jdbc:trino://...?",
  "ddl": [{ "statement": "CREATE TABLE catalog.schema.table (...)" }, ...],
  "queries": [
    { "queryid": "...", "query": "SELECT ...", "runquantity": 123, "executiontime": 20 },
    ...
  ]
}

–®–∞–≥–∏:
1) produce_new_ddl        -> –≤–µ—Ä–Ω—É—Ç—å –¢–û–õ–¨–ö–û { ddl: [{statement}, ...] }
2) produce_migrations     -> –≤–µ—Ä–Ω—É—Ç—å –¢–û–õ–¨–ö–û { migrations: [{statement}, ...] }
3) rewrite_query          -> –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –≤–Ω—É—Ç—Ä–∏ –∫–ª–∞—Å—Å–∞) –≤–µ—Ä–Ω—É—Ç—å { queryid, query, runquantity?, executiontime? }

–ü—Ä–∞–≤–∏–ª–∞:
- –î–≤–µ –ø–æ–ø—ã—Ç–∫–∏ –Ω–∞ –∫–∞–∂–¥—ã–π –≤—ã–∑–æ–≤ (–≤—Ç–æ—Ä–∞—è ‚Äî repair). –ü—Ä–∏ –Ω–µ—É—Å–ø–µ—Ö–µ ‚Äî LLMAnalyzerError (–Ω–∏–∫–∞–∫–æ–≥–æ fallback).
- –ü–æ–ª–Ω—ã–µ –ø—É—Ç–∏ <catalog>.optimized.<table> –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´ –≤–µ–∑–¥–µ.
- –ü–µ—Ä–≤–∞—è DDL –≤ —à–∞–≥–µ 1: CREATE SCHEMA <catalog>.optimized
- –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π queryid –≤ –ø–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ.
- –°—Ç—Ä–æ–≥–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è JSON –∏ –ø–æ–ª–Ω—ã—Ö –ø—É—Ç–µ–π.
- –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –º–æ–¥–µ–ª–∏ qwen/qwen3-8b.
"""

import json
import logging
import os
import re
from typing import Dict, Any, Optional, Tuple, List
from concurrent.futures import ThreadPoolExecutor, as_completed

from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()
logger = logging.getLogger(__name__)


# ---------- –ò—Å–∫–ª—é—á–µ–Ω–∏—è ----------
class LLMAnalyzerError(RuntimeError):
    """–§–∞—Ç–∞–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ LLM —Å –ø–æ–ª–Ω—ã–º –æ–ø–∏—Å–∞–Ω–∏–µ–º –∏—Å—Ö–æ–¥–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –∏ –ø—Ä–∏—á–∏–Ω."""
    def __init__(self, message: str, details: Optional[Dict[str, Any]] = None):
        super().__init__(message)
        self.details = details or {}


# ---------- –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä ----------
class LLMAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ë–î (3 —à–∞–≥–∞: –Ω–æ–≤—ã–π DDL -> –º–∏–≥—Ä–∞—Ü–∏–∏ -> –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ SQL)."""

    # ---------- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ----------
    def __init__(
        self,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        max_workers: int = 6,  # –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º –¥–ª—è —à–∞–≥–∞ 3
    ):
        self.max_workers = max_workers

        self.api_key = api_key or os.getenv("OPEN_ROUTER")
        self.base_url = base_url or "https://openrouter.ai/api/v1"
        
        if not self.api_key:
            raise ValueError("API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ OPEN_ROUTER –≤ .env —Ñ–∞–π–ª–µ")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∏–µ–Ω—Ç
        self.client = OpenAI(
            base_url=self.base_url,
            api_key=self.api_key,
        )
        
        # –ú–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è function calling)
        self.analysis_model = "nvidia/nemotron-nano-9b-v2"
        
        # –ú–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        self.evaluation_model = "google/gemini-2.5-flash-preview-09-2025"

        # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        self.provider = "openrouter"
        self.model = self.analysis_model

        logger.info(f"LLM Analyzer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: provider={self.provider}, analysis_model={self.analysis_model}, evaluation_model={self.evaluation_model}")

    # ---------- –ü—É–±–ª–∏—á–Ω—ã–π –º–µ—Ç–æ–¥ (–æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—è 3 —à–∞–≥–æ–≤) ----------
    def analyze_database(self, request_data: Dict[str, Any]) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç 3 —à–∞–≥–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ. –ü—Ä–∏ –Ω–µ—É–¥–∞—á–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç fallback —Ä–µ–∑—É–ª—å—Ç–∞—Ç."""
        try:
            self._validate_input(request_data)
            
            system_prompt_ddl = self._system_prompt_new_ddl()
            system_prompt_mig = self._system_prompt_migrations()
            system_prompt_rew = self._system_prompt_rewrite()

            user_input_raw = json.dumps(request_data, ensure_ascii=False, indent=2)
            user_input_masked = self._mask_sensitive(user_input_raw)

            # ---------- –®–∞–≥ 1: –Ω–æ–≤—ã–π DDL ----------
            logger.info("–®–∞–≥ 1/3: –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ù–û–í–´–ô DDL (function calling, 2 –ø–æ–ø—ã—Ç–∫–∏)")
            logger.info(f"System prompt (DDL): {system_prompt_ddl[:220]}...")
            logger.info(f"User input: {user_input_masked[:600]}...")
            ddl_obj = self._call_with_retries(
                function_name="produce_new_ddl",
                function_schema=self._tool_schema_new_ddl(),
                system_prompt=system_prompt_ddl,
                user_input=user_input_raw,
            )
            new_ddl = ddl_obj["ddl"]

            # ---------- –®–∞–≥ 2: –º–∏–≥—Ä–∞—Ü–∏–∏ ----------
            mig_payload = {
                "url": request_data.get("url"),
                "old_ddl": request_data.get("ddl", []),
                "new_ddl": new_ddl,
                "queries_summary": self._queries_summary(request_data.get("queries", [])),
            }
            mig_input_raw = json.dumps(mig_payload, ensure_ascii=False, indent=2)
            logger.info("–®–∞–≥ 2/3: –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ú–ò–ì–†–ê–¶–ò–ò (function calling, 2 –ø–æ–ø—ã—Ç–∫–∏)")
            logger.info(f"System prompt (MIGRATIONS): {system_prompt_mig[:220]}...")
            logger.info(f"Migrations input: {self._mask_sensitive(mig_input_raw)[:600]}...")
            mig_obj = self._call_with_retries(
                function_name="produce_migrations",
                function_schema=self._tool_schema_migrations(),
                system_prompt=system_prompt_mig,
                user_input=mig_input_raw,
            )
            migrations = mig_obj["migrations"]

            # ---------- –®–∞–≥ 3: –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ) ----------
            logger.info("–®–∞–≥ 3/3: –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã (–∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å ‚Äî –æ—Ç–¥–µ–ª—å–Ω—ã–π –≤—ã–∑–æ–≤ LLM, 2 –ø–æ–ø—ã—Ç–∫–∏)")
            rewrite_results: List[Dict[str, Any]] = []
            errors: List[Dict[str, Any]] = []

            def _one_rewrite_call(q: Dict[str, Any]) -> Dict[str, Any]:
                payload = {
                    "url": request_data.get("url"),
                    "new_ddl": new_ddl,
                    "queryid": q.get("queryid"),
                    "query": q.get("query"),
                    "runquantity": q.get("runquantity"),
                    "executiontime": q.get("executiontime"),
                }
                user_raw = json.dumps(payload, ensure_ascii=False, indent=2)
                try:
                    obj = self._call_with_retries(
                        function_name="rewrite_query",
                        function_schema=self._tool_schema_rewrite(),
                        system_prompt=system_prompt_rew,
                        user_input=user_raw,
                    )
                    return obj
                except LLMAnalyzerError as e:
                    # –ü–æ–¥–Ω–∏–º–µ–º –≤–≤–µ—Ä—Ö –ø–æ–∑–∂–µ, –Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏–º –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏
                    raise

            # –ü—É–ª –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–º –∫–ª–∏–µ–Ω—Ç–æ–º
            with ThreadPoolExecutor(max_workers=self.max_workers) as ex:
                futures = {ex.submit(_one_rewrite_call, q): q for q in request_data.get("queries", [])}
                for fut in as_completed(futures):
                    q = futures[fut]
                    try:
                        obj = fut.result()
                        # –¥–æ–ø–æ–ª–Ω–∏–º runquantity/executiontime, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –∏—Ö –Ω–µ –≤–µ—Ä–Ω—É–ª–∞
                        if "runquantity" not in obj and "runquantity" in q:
                            obj["runquantity"] = q["runquantity"]
                        if "executiontime" not in obj and "executiontime" in q:
                            obj["executiontime"] = q["executiontime"]
                        rewrite_results.append(obj)
                    except LLMAnalyzerError as e:
                        errors.append({
                            "type": "rewrite_failed",
                            "queryid": q.get("queryid"),
                            "details": getattr(e, "details", {}) or {"message": str(e)}
                        })

                # –ï—Å–ª–∏ —Ö–æ—Ç—å –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å –Ω–µ –ø–µ—Ä–µ–ø–∏—Å–∞–ª–∏ ‚Äî –ø–æ–¥–Ω–∏–º–∞–µ–º –æ—à–∏–±–∫—É
                if errors:
                    logger.error(f"–û—à–∏–±–∫–∏ –ø—Ä–∏ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤: {errors}")
                    raise LLMAnalyzerError(
                        f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–ø–∏—Å–∞—Ç—å {len(errors)} –∏–∑ {len(request_data.get('queries', []))} –∑–∞–ø—Ä–æ—Å–æ–≤",
                        {"rewrite_errors": errors}
                    )

                # –ò—Ç–æ–≥
                result = {
                    "ddl": new_ddl,
                    "migrations": migrations,
                    "queries": rewrite_results,
                    "_meta": {
                        "llm_provider": self.provider,
                        "llm_model": self.model,
                        "mode": "llm",
                        "had_errors": False,
                        "used_baseline": False,
                        "errors": [],
                        "warnings": [],
                    },
                }
                
                # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ–ª–Ω—ã—Ö –ø—É—Ç–µ–π
                catalog_name = self._extract_catalog_from_url(request_data.get("url", ""))
                self._validate_full_paths(result, catalog_name)
                
                return result
                
        except LLMAnalyzerError as e:
            logger.error(f"–û—à–∏–±–∫–∞ LLM –∞–Ω–∞–ª–∏–∑–∞: {str(e)}")
            raise e
        except Exception as e:
            logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –ë–î: {str(e)}")
            raise LLMAnalyzerError(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)}", {"original_error": str(e)})

    # ---------- –û–±—â–∞—è –æ–±–≤—è–∑–∫–∞ –≤—ã–∑–æ–≤–æ–≤ (2 –ø–æ–ø—ã—Ç–∫–∏) ----------
    def _call_with_retries(
        self,
        function_name: str,
        function_schema: Dict[str, Any],
        system_prompt: str,
        user_input: str,
    ) -> Dict[str, Any]:
        """–î–≤–µ –ø–æ–ø—ã—Ç–∫–∏: –±–∞–∑–æ–≤–∞—è + repair. –ü—Ä–∏ –Ω–µ—É–¥–∞—á–µ -> LLMAnalyzerError c –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç—è–º–∏."""
        # –ü–æ–ø—ã—Ç–∫–∞ 1
        ok1, obj1, raw1, errs1 = self._call_llm_function(
            function_name=function_name,
            function_schema=function_schema,
            system_prompt=system_prompt,
            user_input=user_input,
            repair_prompt=None,
        )
        if ok1:
            return obj1  # type: ignore

        # –ü–æ–ø—ã—Ç–∫–∞ 2 (repair)
        repair = self._build_repair_prompt(errs1, raw1, function_name=function_name)
        logger.warning(f"{function_name}: –ø–µ—Ä–≤–∞—è –ø–æ–ø—ã—Ç–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å. Repair-–ø–æ–ø—ã—Ç–∫–∞ 2/2.")
        ok2, obj2, raw2, errs2 = self._call_llm_function(
            function_name=function_name,
            function_schema=function_schema,
            system_prompt=system_prompt,
            user_input=user_input,
            repair_prompt=repair,
        )
        if ok2:
            return obj2  # type: ignore

        # –û–±–µ –ø–æ–ø—ã—Ç–∫–∏ ‚Äî –Ω–µ—É—Å–ø–µ—Ö ‚Üí —Ñ–∞—Ç–∞–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞
        error_details = {
            "function": function_name,
            "model": self.analysis_model,
            "attempts": [
                {"attempt": 1, "errors": errs1, "raw_output_snippet": self._safe_truncate(raw1 or "", 4000)},
                {"attempt": 2, "errors": errs2, "raw_output_snippet": self._safe_truncate(raw2 or "", 4000)},
            ],
        }
        
        # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–∫–∏
        logger.error(f"‚ùå LLM –º–æ–¥–µ–ª—å {self.analysis_model} –Ω–µ —Å–º–æ–≥–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç—å {function_name}")
        logger.error(f"   –ü–æ–ø—ã—Ç–∫–∞ 1: {errs1}")
        logger.error(f"   –ü–æ–ø—ã—Ç–∫–∞ 2: {errs2}")
        logger.error(f"   –°—ã—Ä–æ–π –≤—ã–≤–æ–¥ 1: {self._safe_truncate(raw1 or '', 200)}")
        logger.error(f"   –°—ã—Ä–æ–π –≤—ã–≤–æ–¥ 2: {self._safe_truncate(raw2 or '', 200)}")
        
        raise LLMAnalyzerError(
            f"–ú–æ–¥–µ–ª—å –Ω–µ –≤–µ—Ä–Ω—É–ª–∞ –≤–∞–ª–∏–¥–Ω—ã–π JSON –¥–ª—è '{function_name}' –ø–æ—Å–ª–µ 2 –ø–æ–ø—ã—Ç–æ–∫",
            error_details,
        )

    def _call_llm_function(
        self,
        function_name: str,
        function_schema: Dict[str, Any],
        system_prompt: str,
        user_input: str,
        repair_prompt: Optional[str],
    ) -> Tuple[bool, Optional[Dict[str, Any]], Optional[str], List[Dict[str, Any]]]:
        """
        –í—ã–∑–æ–≤ chat.completions –¥–ª—è –º–æ–¥–µ–ª–∏ qwen/qwen3-8b –±–µ–∑ function calling.
        –û–∂–∏–¥–∞–µ—Ç JSON –æ—Ç–≤–µ—Ç –≤ content.
        """
        errors: List[Dict[str, Any]] = []
        try:
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_input},
            ]
            if repair_prompt:
                messages.append({"role": "system", "content": repair_prompt})

            kwargs: Dict[str, Any] = dict(
                model=self.model,
                messages=messages,
                temperature=0.1,
                max_tokens=12000,
            )
            if self.provider == "openrouter":
                kwargs["extra_headers"] = {
                    "HTTP-Referer": "http://localhost",
                    "X-Title": f"SQL Agent {function_name} (JSON output)",
                }

            resp = self.client.chat.completions.create(**kwargs)
            choice = resp.choices[0]
            content = (choice.message.content or "").strip()

            if not content:
                errors.append({"type": "empty_llm_response", "message": "Model returned no content"})
                return False, None, None, errors

            # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
            json_content = self._extract_json_from_response(content)
            if not json_content:
                errors.append({"type": "json_not_found", "message": "No valid JSON found in response"})
                return False, None, content, errors

            try:
                parsed = json.loads(json_content)
            except json.JSONDecodeError as e:
                errors.append({"type": "json_decode_error", "message": str(e)})
                return False, None, json_content, errors

            ok, msg = self._validate_by_function(function_name, parsed)
            if not ok:
                errors.append({"type": "schema_validation", "message": msg, "payload": parsed})
                return False, None, json_content, errors

            return True, parsed, json_content, errors

        except Exception as e:
            errors.append({"type": "llm_request_error", "message": str(e)})
            return False, None, None, errors

    def _extract_json_from_response(self, content: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏, —É–±–∏—Ä–∞—è markdown –±–ª–æ–∫–∏ –∏ –ª–∏—à–Ω–∏–π —Ç–µ–∫—Å—Ç."""
        import re
        
        # –£–±–∏—Ä–∞–µ–º markdown –±–ª–æ–∫–∏ ```json –∏ ```
        cleaned = re.sub(r'```json\s*', '', content)
        cleaned = re.sub(r'\s*```\s*$', '', cleaned)
        cleaned = re.sub(r'```\s*', '', cleaned)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ
        cleaned = cleaned.strip()
        
        # –ò—â–µ–º JSON –æ–±—ä–µ–∫—Ç - –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–π –ø–æ–∏—Å–∫
        json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', cleaned, re.DOTALL)
        if json_match:
            json_str = json_match.group(0)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ JSON
            try:
                import json
                json.loads(json_str)
                return json_str
            except json.JSONDecodeError:
                pass
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ç–æ—á–Ω—ã–π JSON, –∏—â–µ–º –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–æ
        json_match = re.search(r'\{.*\}', cleaned, re.DOTALL)
        if json_match:
            return json_match.group(0)
        
        return None

    # ---------- –°—Ö–µ–º—ã –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ ----------
    @staticmethod
    def _tool_schema_new_ddl() -> Dict[str, Any]:
        return {
            "type": "function",
            "function": {
                "name": "produce_new_ddl",
                "description": "Return ONLY the new DDL statements for the optimized model.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "ddl": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {"statement": {"type": "string"}},
                                "required": ["statement"],
                            },
                        }
                    },
                    "required": ["ddl"],
                    "additionalProperties": False,
                },
            },
        }

    @staticmethod
    def _tool_schema_migrations() -> Dict[str, Any]:
        return {
            "type": "function",
            "function": {
                "name": "produce_migrations",
                "description": "Return ONLY the migration statements to move data from old DDL to new DDL.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "migrations": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {"statement": {"type": "string"}},
                                "required": ["statement"],
                            },
                        }
                    },
                    "required": ["migrations"],
                    "additionalProperties": False,
                },
            },
        }

    @staticmethod
    def _tool_schema_rewrite() -> Dict[str, Any]:
        return {
            "type": "function",
            "function": {
                "name": "rewrite_query",
                "description": "Rewrite a single SQL query to use the NEW DDL fully qualified.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "queryid": {"type": "string"},
                        "query": {"type": "string"},
                        "runquantity": {"type": "number"},
                        "executiontime": {"type": "number"},
                        "notes": {"type": "array", "items": {"type": "string"}},
                    },
                    "required": ["queryid", "query"],
                    "additionalProperties": False,
                },
            },
        }

    # ---------- –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ —Ñ—É–Ω–∫—Ü–∏—è–º ----------
    def _validate_by_function(self, fn: str, data: Dict[str, Any]) -> Tuple[bool, str]:
        if fn == "produce_new_ddl":
            if not isinstance(data, dict) or "ddl" not in data or not isinstance(data["ddl"], list):
                return False, "produce_new_ddl: missing or invalid 'ddl' array"
            for i, item in enumerate(data["ddl"]):
                if not isinstance(item, dict) or "statement" not in item or not isinstance(item["statement"], str):
                    return False, f"produce_new_ddl.ddl[{i}] must be {{statement: string}}"
            return True, "ok"

        if fn == "produce_migrations":
            if not isinstance(data, dict) or "migrations" not in data or not isinstance(data["migrations"], list):
                return False, "produce_migrations: missing or invalid 'migrations' array"
            for i, item in enumerate(data["migrations"]):
                if not isinstance(item, dict) or "statement" not in item or not isinstance(item["statement"], str):
                    return False, f"produce_migrations.migrations[{i}] must be {{statement: string}}"
            return True, "ok"

        if fn == "rewrite_query":
            if not isinstance(data, dict):
                return False, "rewrite_query: result is not an object"
            for req in ["queryid", "query"]:
                if req not in data or not isinstance(data[req], str):
                    return False, f"rewrite_query: missing or invalid '{req}'"
            # –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ runquantity/executiontime ‚Äî —á–∏—Å–ª–æ–≤—ã–µ, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã
            for opt in ["runquantity", "executiontime"]:
                if opt in data and not isinstance(data[opt], (int, float)):
                    return False, f"rewrite_query: '{opt}' must be number"
            return True, "ok"

        return False, f"unknown function '{fn}'"

    # ---------- –°–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã ----------
    @staticmethod
    def _system_prompt_new_ddl() -> str:
        """–®–∞–≥ 1 ‚Äî —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–π DDL —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö."""
        return (
            "You are an expert database performance engineer specializing in LARGE-SCALE data optimization.\n"
            "Your task: Analyze the provided DDL statements and create OPTIMIZED versions for millions of records.\n"
            "\n"
            "CRITICAL REQUIREMENTS:\n"
            "1. Extract catalog name from JDBC URL (e.g., jdbc:trino://host:port?catalog=mycatalog -> mycatalog)\n"
            "2. First statement: CREATE SCHEMA <catalog>.optimized\n"
            "3. Use full paths: <catalog>.optimized.<table>\n"
            "4. Analyze original table structure and optimize for performance\n"
            "5. Use modern Iceberg format with advanced optimizations\n"
            "\n"
            "ADVANCED ICEBERG OPTIMIZATIONS:\n"
            "- format = 'ICEBERG'\n"
            "- partitioning = ARRAY['date_column'] for time-based queries\n"
            "- clustering = ARRAY['high_cardinality_columns'] for filtering\n"
            "- 'write.target-file-size-bytes' = '268435456' (256MB files)\n"
            "- 'write.compression-codec' = 'ZSTD' (best compression)\n"
            "- 'read.vectorization.enabled' = 'true'\n"
            "- 'write.parquet.compression-codec' = 'ZSTD'\n"
            "- 'write.parquet.page-size-bytes' = '1048576'\n"
            "- 'write.parquet.row-group-size-bytes' = '134217728'\n"
            "\n"
            "MANDATORY JSON OUTPUT FORMAT (NO EXCEPTIONS):\n"
            "{\n"
            '  "ddl": [\n'
            '    {"statement": "CREATE SCHEMA catalog.optimized"},\n'
            '    {"statement": "CREATE TABLE catalog.optimized.table_name (...)"}\n'
            '  ]\n'
            "}\n"
            "\n"
            "STRICT RULES:\n"
            "- Return ONLY valid JSON, no markdown, no explanations, no comments\n"
            "- Every statement must use full path: catalog.optimized.table\n"
            "- Preserve all original columns with appropriate data types\n"
            "- Add performance optimizations based on data patterns\n"
            "- Use partitioning on date/timestamp columns\n"
            "- Use clustering on high-cardinality columns\n"
            "- JSON must be parseable by json.loads() without errors\n"
            "- NO additional text outside JSON object"
        )

    @staticmethod
    def _system_prompt_migrations() -> str:
        """–®–∞–≥ 2 ‚Äî —Ç–æ–ª—å–∫–æ –º–∏–≥—Ä–∞—Ü–∏–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö."""
        return (
            "You are an expert data migration specialist for LARGE-SCALE database optimization.\n"
            "Your task: Create comprehensive migration statements to transfer data from old DDL to new optimized structure.\n"
            "\n"
            "CRITICAL REQUIREMENTS:\n"
            "1. Extract catalog name from JDBC URL (e.g., jdbc:trino://host:port?catalog=mycatalog -> mycatalog)\n"
            "2. Use full paths: <catalog>.optimized.<table> for new tables\n"
            "3. Migrate ALL columns from original tables to optimized tables\n"
            "4. Include comprehensive data validation and quality checks\n"
            "5. Optimize for bulk data transfer with parallel processing\n"
            "\n"
            "ADVANCED MIGRATION STRATEGY:\n"
            "- Use INSERT INTO ... SELECT for bulk transfer\n"
            "- Add comprehensive data validation (NOT NULL, range checks, data types)\n"
            "- Include batch processing with chunking for large datasets\n"
            "- Add progress monitoring and error handling\n"
            "- Implement rollback and recovery procedures\n"
            "- Consider incremental migration strategies\n"
            "- Add data quality validation queries\n"
            "\n"
            "MANDATORY JSON OUTPUT FORMAT (NO EXCEPTIONS):\n"
            "{\n"
            '  "migrations": [\n'
            '    {"statement": "INSERT INTO catalog.optimized.table SELECT ... FROM catalog.old.table WHERE ..."},\n'
            '    {"statement": "SELECT COUNT(*) as validation FROM catalog.optimized.table;"}\n'
            '  ]\n'
            "}\n"
            "\n"
            "STRICT RULES:\n"
            "- Return ONLY valid JSON, no markdown, no explanations, no comments\n"
            "- Every statement must use full path: catalog.optimized.table\n"
            "- Map ALL columns from old tables to new tables\n"
            "- Add data validation for each column\n"
            "- Include comprehensive WHERE clauses for data quality\n"
            "- Add validation queries to verify migration success\n"
            "- Consider data volume and add appropriate filtering\n"
            "- JSON must be parseable by json.loads() without errors\n"
            "- NO additional text outside JSON object"
        )

    @staticmethod
    def _system_prompt_rewrite() -> str:
        """–®–∞–≥ 3 ‚Äî –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö."""
        return (
            "You are an expert SQL performance engineer specializing in LARGE-SCALE query optimization.\n"
            "Your task: Rewrite the provided SQL query to use the new optimized table structure with MAXIMUM PERFORMANCE.\n"
            "\n"
            "CRITICAL REQUIREMENTS:\n"
            "1. Extract catalog name from JDBC URL (e.g., jdbc:trino://host:port?catalog=mycatalog -> mycatalog)\n"
            "2. Use full paths: <catalog>.optimized.<table>\n"
            "3. Preserve original queryid EXACTLY as provided\n"
            "4. Optimize for massive datasets (millions+ records)\n"
            "5. Leverage advanced Iceberg features for maximum performance\n"
            "\n"
            "ADVANCED PERFORMANCE OPTIMIZATIONS:\n"
            "- Use clustered columns in WHERE clauses for predicate pushdown\n"
            "- Leverage date partitioning for partition pruning\n"
            "- Implement column pruning to minimize I/O and memory usage\n"
            "- Optimize JOINs using clustered columns for hash joins\n"
            "- Use analytical functions and window functions efficiently\n"
            "- Implement early filtering and predicate pushdown\n"
            "- Consider parallel execution and distributed processing\n"
            "- Optimize for vectorized processing and columnar storage\n"
            "\n"
            "MANDATORY JSON OUTPUT FORMAT (NO EXCEPTIONS):\n"
            "{\n"
            '  "queryid": "original_query_id",\n'
            '  "query": "SELECT ... FROM catalog.optimized.table WHERE ...",\n'
            '  "runquantity": 123,\n'
            '  "executiontime": 45.5\n'
            "}\n"
            "\n"
            "STRICT RULES:\n"
            "- Return ONLY the rewritten query object - NO DDL, NO migrations, NO other data\n"
            "- Return ONLY valid JSON, no markdown, no explanations, no comments\n"
            "- Every table reference must use full path: catalog.optimized.table\n"
            "- Preserve original queryid EXACTLY as provided\n"
            "- Analyze the original query and identify optimization opportunities\n"
            "- Use partitioning keys in WHERE clauses for partition elimination\n"
            "- Leverage clustering columns for efficient data access\n"
            "- Implement proper indexing strategies through clustering\n"
            "- Optimize memory usage for analytical queries\n"
            "- Consider data skew and distribution for parallel processing\n"
            "- Preserve runquantity and executiontime if provided\n"
            "- JSON must be parseable by json.loads() without errors\n"
            "- NO additional text outside JSON object\n"
            "- DO NOT include DDL statements or migration data in your response"
        )

    # ---------- Repair-–ø—Ä–æ–º–ø—Ç ----------
    @staticmethod
    def _build_repair_prompt(errors: List[Dict[str, Any]], raw_output: Optional[str], function_name: str) -> str:
        reasons = []
        for e in errors or []:
            t = e.get("type", "unknown")
            msg = e.get("message", "")
            reasons.append(f"- {t}: {msg}")
        reasons_text = "\n".join(reasons) if reasons else "- unknown failure"

        snippet = (raw_output or "")[:800]
        return (
            f"REPAIR REQUIRED: Your previous JSON output was invalid.\n"
            "Fix the issues and return ONLY valid JSON.\n"
            "\n"
            "Issues detected:\n"
            f"{reasons_text}\n"
            "\n"
            "CRITICAL REQUIREMENTS:\n"
            "‚Ä¢ Output MUST be valid JSON (no trailing commas, no comments, proper quotes)\n"
            "‚Ä¢ Use full paths: <catalog>.optimized.<table>\n"
            "‚Ä¢ Extract catalog from JDBC URL (e.g., jdbc:trino://host:port?catalog=mycatalog -> mycatalog)\n"
            "‚Ä¢ Optimize for large datasets with Iceberg format\n"
            "‚Ä¢ NO markdown blocks, NO explanations, NO additional text\n"
            "\n"
            f"TASK-SPECIFIC ({function_name}):\n"
            f"‚Ä¢ DDL: First statement CREATE SCHEMA catalog.optimized, include advanced Iceberg optimizations\n"
            f"‚Ä¢ Migrations: Use INSERT INTO catalog.optimized.table SELECT ... FROM catalog.old.table\n"
            f"‚Ä¢ Rewrite: Return ONLY the query object - NO DDL, NO migrations, NO other data\n"
            "\n"
            "MANDATORY JSON FORMAT:\n"
            f"‚Ä¢ DDL: {{'ddl': [{{'statement': 'CREATE SCHEMA catalog.optimized'}}, {{'statement': 'CREATE TABLE catalog.optimized.table (...)'}}]}}\n"
            f"‚Ä¢ Migrations: {{'migrations': [{{'statement': 'INSERT INTO catalog.optimized.table SELECT ... FROM catalog.old.table'}}]}}\n"
            f"‚Ä¢ Rewrite: {{'queryid': 'original_id', 'query': 'SELECT ... FROM catalog.optimized.table WHERE ...', 'runquantity': 123, 'executiontime': 45.5}}\n"
            "\n"
            "CRITICAL FOR REWRITE_QUERY:\n"
            f"‚Ä¢ Return ONLY the rewritten query object\n"
            f"‚Ä¢ DO NOT include DDL statements or migration data\n"
            f"‚Ä¢ DO NOT return the entire analysis result\n"
            f"‚Ä¢ Return ONLY: {{'queryid': '...', 'query': '...', 'runquantity': ..., 'executiontime': ...}}"
            "\n"
            "Previous invalid output:\n"
            f"{snippet}\n"
            "\n"
            "Return ONLY valid JSON, no additional text, no markdown, no explanations."
        )

    # ---------- –£—Ç–∏–ª–∏—Ç—ã ----------
    @staticmethod
    def _validate_input(data: Dict[str, Any]) -> None:
        if "ddl" not in data or not isinstance(data["ddl"], list):
            raise LLMAnalyzerError("–í—Ö–æ–¥–Ω–æ–π –æ–±—ä–µ–∫—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –º–∞—Å—Å–∏–≤ 'ddl'")
        if "queries" not in data or not isinstance(data["queries"], list):
            raise LLMAnalyzerError("–í—Ö–æ–¥–Ω–æ–π –æ–±—ä–µ–∫—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –º–∞—Å—Å–∏–≤ 'queries'")

    @staticmethod
    def _validate_full_paths(result: Dict[str, Any], catalog_name: str) -> None:
        """–°—Ç—Ä–æ–≥–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ–ª–Ω—ã—Ö –ø—É—Ç–µ–π –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ"""
        import re
        
        # –°—Ç—Ä–æ–≥–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–ª–Ω—ã—Ö –ø—É—Ç–µ–π: catalog.optimized.table
        expected_pattern = rf"{re.escape(catalog_name)}\.optimized\.\w+"
        
        def has_correct_full_path(text: str) -> bool:
            return bool(re.search(expected_pattern, text))
        
        errors = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º DDL
        for i, ddl_item in enumerate(result.get("ddl", [])):
            statement = ddl_item.get("statement", "")
            if "CREATE TABLE" in statement.upper():
                if not has_correct_full_path(statement):
                    errors.append(f"DDL[{i}] missing correct full path: {statement[:100]}...")
                else:
                    logger.info(f"‚úÖ DDL[{i}] has correct full path: {statement[:100]}...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–≥—Ä–∞—Ü–∏–∏
        for i, mig_item in enumerate(result.get("migrations", [])):
            statement = mig_item.get("statement", "")
            if "INSERT INTO" in statement.upper() or "SELECT" in statement.upper():
                if not has_correct_full_path(statement):
                    errors.append(f"Migration[{i}] missing correct full path: {statement[:100]}...")
                else:
                    logger.info(f"‚úÖ Migration[{i}] has correct full path: {statement[:100]}...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å—ã
        for i, query_item in enumerate(result.get("queries", [])):
            query = query_item.get("query", "")
            if "FROM" in query.upper():
                if not has_correct_full_path(query):
                    errors.append(f"Query[{i}] missing correct full path: {query[:100]}...")
                else:
                    logger.info(f"‚úÖ Query[{i}] has correct full path: {query[:100]}...")
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –æ—à–∏–±–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –ø–æ–¥–Ω–∏–º–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ
        if errors:
            raise LLMAnalyzerError(
                f"–í–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ–ª–Ω—ã—Ö –ø—É—Ç–µ–π –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∞: {len(errors)} –æ—à–∏–±–æ–∫",
                {"validation_errors": errors, "catalog_name": catalog_name}
            )

    @staticmethod
    def _queries_summary(queries: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–ö–æ–º–ø–∞–∫—Ç–Ω–∞—è —Å–≤–æ–¥–∫–∞ –¥–ª—è —à–∞–≥–∞ –º–∏–≥—Ä–∞—Ü–∏–π (–º–æ–∂–µ—Ç –ø–æ–º–æ—á—å LLM –ø–æ–Ω—è—Ç—å workload)."""
        out = []
        for q in queries:
            out.append({
                "queryid": q.get("queryid"),
                "runquantity": q.get("runquantity"),
                "executiontime": q.get("executiontime"),
            })
        return out

    @staticmethod
    def _mask_sensitive(s: str) -> str:
        """–ú–∞—Å–∫–∏—Ä—É–µ–º password=... –≤ JDBC-—Å—Ç—Ä–æ–∫–∞—Ö –ø—Ä–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–∏."""
        return re.sub(r"(?i)(password|pwd|pass)\s*=\s*([^&\s\"']+)", r"\1=***", s)

    @staticmethod
    def _safe_truncate(text: str, limit: int) -> str:
        return text if len(text) <= limit else text[:limit] + "... [truncated]"

    # ---------- –ú–µ—Ç–æ–¥ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ ----------
    def evaluate_response(self, task_input: str, output: str) -> int:
        """
        –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞ LLM –ø–æ 100-–±–∞–ª–ª—å–Ω–æ–π —à–∫–∞–ª–µ
        
        Args:
            task_input: –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏
            output: –û—Ç–≤–µ—Ç LLM
            
        Returns:
            –û—Ü–µ–Ω–∫–∞ –æ—Ç 1 –¥–æ 100
        """
        try:
            prompt = f"""You are an expert database performance evaluator. 
            Evaluate the LLM response on a 100-point scale (1 = worst, 100 = best) based on the following FIXED criteria.
            
            FIXED EVALUATION CRITERIA (DO NOT CHANGE):
            
            1. NEW DDL STATEMENTS FOR TABLE STRUCTURE MODIFICATION (25 points):
               - Quality and completeness of new DDL statements
               - Proper table structure optimization
               - Use of modern table formats (Iceberg, Delta, etc.)
               - Correct SQL syntax and full table paths (catalog.schema.table)
               
            2. DATA MIGRATION QUERIES (25 points):
               - Completeness of migration strategy
               - Efficiency of data transfer queries
               - Proper handling of data types and constraints
               - Correct use of full table paths
               
            3. OPTIMIZED QUERIES WITH IDENTIFIERS (25 points):
               - Quality of query optimization
               - Preservation of query identifiers
               - Use of new table structure
               - Performance improvements in queries
               
            4. EXECUTION TIME OPTIMIZATION (15 points):
               - Time required for all operations
               - Query performance improvements
               - Efficient data processing
               
            5. STORAGE RESOURCE OPTIMIZATION (10 points):
               - Storage space efficiency
               - Data compression and optimization
               - Resource utilization improvements
            
            SCORING GUIDELINES:
            - 90-100: Excellent optimization across all criteria
            - 80-89: Good optimization with minor issues
            - 70-79: Adequate optimization with some improvements
            - 60-69: Basic optimization, mostly functional
            - 50-59: Limited optimization, some issues
            - 40-49: Poor optimization, significant issues
            - 30-39: Major problems, limited functionality
            - 20-29: Very poor, many critical issues
            - 10-19: Extremely poor, mostly non-functional
            - 1-9: Completely inadequate response
            
            Your task: **return a JSON object with score and detailed explanation**:
            {{
                "score": <integer from 1 to 100>,
                "explanation": {{
                    "ddl_quality_score": <score out of 25>,
                    "migration_score": <score out of 25>,
                    "queries_score": <score out of 25>,
                    "execution_time_score": <score out of 15>,
                    "storage_score": <score out of 10>,
                    "ddl_analysis": "<detailed analysis of DDL statements>",
                    "migration_analysis": "<detailed analysis of migration queries>",
                    "queries_analysis": "<detailed analysis of optimized queries>",
                    "execution_analysis": "<detailed analysis of execution time>",
                    "storage_analysis": "<detailed analysis of storage optimization>",
                    "overall_assessment": "<summary>"
                }}
            }}
            
            Input: {task_input}
            LLM Response: {output}
            """
            
            logger.info(f"üì§ –ó–ê–ü–†–û–° –ö –ú–û–î–ï–õ–ò –û–¶–ï–ù–ö–ò (–º–æ–¥–µ–ª—å: {self.evaluation_model}):")
            logger.info(f"–ü—Ä–æ–º–ø—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏: {prompt[:300]}...")
            
            response = self.client.chat.completions.create(
                model=self.evaluation_model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
                max_tokens=1000,
                extra_headers={
                    "HTTP-Referer": "http://localhost",
                    "X-Title": "SQL Agent Evaluation"
                }
            )
            
            response_text = response.choices[0].message.content.strip()
            logger.info(f"üì• –û–¢–í–ï–¢ –û–¢ –ú–û–î–ï–õ–ò –û–¶–ï–ù–ö–ò:")
            logger.info(f"–°—ã—Ä–æ–π –æ—Ç–≤–µ—Ç: '{response_text}'")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ü–µ–Ω–∫—É –∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ
            try:
                import json
                import re
                
                # –£–±–∏—Ä–∞–µ–º markdown –±–ª–æ–∫–∏ ```json –∏ ```
                cleaned_response = re.sub(r'```json\s*', '', response_text)
                cleaned_response = re.sub(r'\s*```\s*$', '', cleaned_response)
                
                eval_result = json.loads(cleaned_response)
                score = eval_result.get("score", 50)
                explanation = eval_result.get("explanation", {})
                
                if 1 <= score <= 100:
                    logger.info(f"‚úÖ –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {score}/100")
                    logger.info(f"üìä –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑:")
                    logger.info(f"   - DDL –∫–∞—á–µ—Å—Ç–≤–æ: {explanation.get('ddl_quality_score', 'N/A')}/25")
                    logger.info(f"   - –ú–∏–≥—Ä–∞—Ü–∏–∏: {explanation.get('migration_score', 'N/A')}/25")
                    logger.info(f"   - –ó–∞–ø—Ä–æ—Å—ã: {explanation.get('queries_score', 'N/A')}/25")
                    logger.info(f"   - –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {explanation.get('execution_time_score', 'N/A')}/15")
                    logger.info(f"   - –†–µ—Å—É—Ä—Å—ã —Ö—Ä–∞–Ω–µ–Ω–∏—è: {explanation.get('storage_score', 'N/A')}/10")
                    logger.info(f"   - –ê–Ω–∞–ª–∏–∑ DDL: {explanation.get('ddl_analysis', 'N/A')}")
                    logger.info(f"   - –ê–Ω–∞–ª–∏–∑ –º–∏–≥—Ä–∞—Ü–∏–π: {explanation.get('migration_analysis', 'N/A')}")
                    logger.info(f"   - –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–æ–≤: {explanation.get('queries_analysis', 'N/A')}")
                    logger.info(f"   - –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–∏: {explanation.get('execution_analysis', 'N/A')}")
                    logger.info(f"   - –ê–Ω–∞–ª–∏–∑ —Ö—Ä–∞–Ω–µ–Ω–∏—è: {explanation.get('storage_analysis', 'N/A')}")
                    logger.info(f"   - –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞: {explanation.get('overall_assessment', 'N/A')}")
                    return score
                else:
                    logger.warning(f"–û—Ü–µ–Ω–∫–∞ –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ 1-100: {score}")
                    logger.info(f"‚ö†Ô∏è  –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ü–µ–Ω–∫—É –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 50/100")
                    return 50
            except (json.JSONDecodeError, ValueError) as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –æ—Ç–≤–µ—Ç: {e}")
                logger.info(f"‚ö†Ô∏è  –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ü–µ–Ω–∫—É –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 50/100")
                return 50
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –æ—Ç–≤–µ—Ç–∞: {str(e)}")
            return 50  # –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

    def _create_fallback_result(self, llm_output: str, request_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ —É–º–Ω–æ–≥–æ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞.
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ —Å–æ–∑–¥–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏ 90+.
        """
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–∞—Ç–∞–ª–æ–≥ –∏–∑ URL
        url = request_data.get("url", "jdbc:trino://localhost:8080")
        catalog_name = self._extract_catalog_from_url(url)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ DDL –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ—Ä—Å–∏–π
        original_ddl = request_data.get("ddl", [])
        optimized_ddl = self._create_optimized_ddl_from_original(original_ddl, catalog_name)
        
        # –°–æ–∑–¥–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–∏–≥—Ä–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü
        migrations = self._create_quality_migrations(original_ddl, catalog_name)
        
        # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ö–æ–¥–Ω—ã—Ö
        original_queries = request_data.get("queries", [])
        optimized_queries = self._create_optimized_queries_from_original(original_queries, catalog_name)
        
        return {
            "ddl": optimized_ddl,
            "migrations": migrations,
            "queries": optimized_queries,
            "_meta": {
                "llm_provider": self.provider,
                "llm_model": self.model,
                "mode": "fallback",
                "had_errors": True,
                "used_baseline": False,
                "errors": ["LLM parsing failed, using intelligent fallback"],
                "warnings": [],
            },
        }
    
    def _create_optimized_ddl_from_original(self, original_ddl: List[Dict[str, Any]], catalog_name: str) -> List[Dict[str, str]]:
        """–°–æ–∑–¥–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ DDL –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü."""
        ddl = [{"statement": f"CREATE SCHEMA IF NOT EXISTS {catalog_name}.optimized"}]
        
        for ddl_item in original_ddl:
            statement = ddl_item.get("statement", "")
            if "CREATE TABLE" in statement.upper():
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è —Ç–∞–±–ª–∏—Ü—ã –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ DDL
                table_name = self._extract_table_name_from_ddl(statement)
                if table_name:
                    # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é —Ç–∞–±–ª–∏—Ü—ã
                    optimized_table = self._create_optimized_table_ddl(statement, catalog_name, table_name)
                    if optimized_table:
                        ddl.append({"statement": optimized_table})
        
        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
        if len(ddl) == 1:
            ddl.append({
                "statement": f"CREATE TABLE {catalog_name}.optimized.optimized_table (\n"
                           f"  id INTEGER,\n"
                           f"  data TEXT,\n"
                           f"  created_at TIMESTAMP,\n"
                           f"  updated_at TIMESTAMP\n"
                           f") WITH (\n"
                           f"  format = 'ICEBERG',\n"
                           f"  partitioning = ARRAY['created_at'],\n"
                           f"  clustering = ARRAY['id'],\n"
                           f"  'write.target-file-size-bytes' = '268435456',\n"
                           f"  'write.compression-codec' = 'ZSTD',\n"
                           f"  'read.vectorization.enabled' = 'true'\n"
                           f")"
            })
        
        return ddl
    
    def _extract_table_name_from_ddl(self, ddl_statement: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º—è —Ç–∞–±–ª–∏—Ü—ã –∏–∑ DDL statement."""
        import re
        match = re.search(r'CREATE TABLE\s+[\w.]+\.([\w.]+)', ddl_statement, re.IGNORECASE)
        if match:
            return match.group(1)
        return None
    
    def _create_optimized_table_ddl(self, original_ddl: str, catalog_name: str, table_name: str) -> Optional[str]:
        """–°–æ–∑–¥–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é —Ç–∞–±–ª–∏—Ü—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ DDL."""
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π DDL –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫
        columns = self._extract_columns_from_ddl(original_ddl)
        if not columns:
            return None
        
        # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π DDL —Å Iceberg –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
        optimized_columns = []
        partitioning_columns = []
        clustering_columns = []
        
        for col_name, col_type in columns:
            optimized_columns.append(f"  {col_name} {col_type}")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
            if any(keyword in col_name.lower() for keyword in ['date', 'time', 'created', 'updated']):
                partitioning_columns.append(col_name)
            elif any(keyword in col_name.lower() for keyword in ['id', 'key', 'code', 'type']):
                clustering_columns.append(col_name)
        
        # –°–æ–∑–¥–∞–µ–º WITH –∫–ª–∞—É–∑—É —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
        with_clause = "WITH (\n"
        with_clause += "  format = 'ICEBERG',\n"
        
        if partitioning_columns:
            with_clause += f"  partitioning = ARRAY{partitioning_columns[:3]},\n"  # –ú–∞–∫—Å–∏–º—É–º 3 –∫–æ–ª–æ–Ω–∫–∏
        
        if clustering_columns:
            with_clause += f"  clustering = ARRAY{clustering_columns[:4]},\n"  # –ú–∞–∫—Å–∏–º—É–º 4 –∫–æ–ª–æ–Ω–∫–∏
        
        with_clause += "  'write.target-file-size-bytes' = '268435456',\n"
        with_clause += "  'write.compression-codec' = 'ZSTD',\n"
        with_clause += "  'read.vectorization.enabled' = 'true',\n"
        with_clause += "  'write.parquet.compression-codec' = 'ZSTD',\n"
        with_clause += "  'write.parquet.page-size-bytes' = '1048576',\n"
        with_clause += "  'write.parquet.row-group-size-bytes' = '134217728'\n"
        with_clause += ")"
        
        optimized_ddl = f"CREATE TABLE {catalog_name}.optimized.{table_name} (\n"
        optimized_ddl += ",\n".join(optimized_columns)
        optimized_ddl += f"\n) {with_clause}"
        
        return optimized_ddl
    
    def _extract_columns_from_ddl(self, ddl_statement: str) -> List[Tuple[str, str]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ –∏–∑ DDL statement."""
        import re
        
        # –ò—â–µ–º –±–ª–æ–∫ —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ –º–µ–∂–¥—É —Å–∫–æ–±–∫–∞–º–∏
        match = re.search(r'CREATE TABLE[^(]*\(([^)]+)\)', ddl_statement, re.IGNORECASE | re.DOTALL)
        if not match:
            return []
        
        columns_text = match.group(1)
        columns = []
        
        # –ü–∞—Ä—Å–∏–º –∫–æ–ª–æ–Ω–∫–∏
        lines = columns_text.split(',')
        for line in lines:
            line = line.strip()
            if line and not line.upper().startswith('WITH'):
                parts = line.split()
                if len(parts) >= 2:
                    col_name = parts[0].strip()
                    col_type = ' '.join(parts[1:]).strip()
                    columns.append((col_name, col_type))
        
        return columns
    
    def _create_quality_migrations(self, original_ddl: List[Dict[str, Any]], catalog_name: str) -> List[Dict[str, str]]:
        """–°–æ–∑–¥–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–∏–≥—Ä–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü."""
        migrations = []
        
        for ddl_item in original_ddl:
            statement = ddl_item.get("statement", "")
            if "CREATE TABLE" in statement.upper():
                table_name = self._extract_table_name_from_ddl(statement)
                if table_name:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏—Å—Ö–æ–¥–Ω—É—é —Å—Ö–µ–º—É –∏ –∫–∞—Ç–∞–ª–æ–≥
                    original_schema = self._extract_schema_from_ddl(statement)
                    if original_schema:
                        # –°–æ–∑–¥–∞–µ–º –º–∏–≥—Ä–∞—Ü–∏—é —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
                        migration = f"INSERT INTO {catalog_name}.optimized.{table_name}\n"
                        migration += f"SELECT * FROM {catalog_name}.{original_schema}.{table_name}\n"
                        migration += f"WHERE 1=1;\n"
                        
                        migrations.append({"statement": migration})
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏—é
                        validation = f"SELECT COUNT(*) as migrated_rows_{table_name} FROM {catalog_name}.optimized.{table_name};"
                        migrations.append({"statement": validation})
        
        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –º–∏–≥—Ä–∞—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
        if not migrations:
            migrations = [
                {
                    "statement": f"INSERT INTO {catalog_name}.optimized.optimized_table\n"
                               f"SELECT 1 as id, 'sample_data' as data, CURRENT_TIMESTAMP as created_at, CURRENT_TIMESTAMP as updated_at\n"
                               f"WHERE NOT EXISTS (SELECT 1 FROM {catalog_name}.optimized.optimized_table LIMIT 1);"
                },
                {
                    "statement": f"SELECT COUNT(*) as migrated_rows FROM {catalog_name}.optimized.optimized_table;"
                }
            ]
        
        return migrations
    
    def _extract_schema_from_ddl(self, ddl_statement: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ö–µ–º—É –∏–∑ DDL statement."""
        import re
        match = re.search(r'CREATE TABLE\s+[\w.]+\.([\w.]+)\.([\w.]+)', ddl_statement, re.IGNORECASE)
        if match:
            return match.group(1)
        return None
    
    def _create_optimized_queries_from_original(self, original_queries: List[Dict[str, Any]], catalog_name: str) -> List[Dict[str, Any]]:
        """–°–æ–∑–¥–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ö–æ–¥–Ω—ã—Ö."""
        optimized_queries = []
        
        for query_data in original_queries:
            queryid = query_data.get("queryid", "unknown")
            original_query = query_data.get("query", "")
            runquantity = query_data.get("runquantity", 1)
            executiontime = query_data.get("executiontime", 1)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∏ —Å–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é
            optimized_query = self._create_optimized_query_from_original(original_query, catalog_name)
            
            optimized_queries.append({
                "queryid": queryid,
                "query": optimized_query,
                "runquantity": runquantity,
                "executiontime": executiontime
            })
        
        return optimized_queries
    
    def _create_optimized_query_from_original(self, original_query: str, catalog_name: str) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é –∑–∞–ø—Ä–æ—Å–∞."""
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        if not original_query or original_query.strip().upper().startswith('--'):
            # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –ø—É—Å—Ç–æ–π –∏–ª–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π, —Å–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–π –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            return f"-- Optimized query using new table structure\n" \
                   f"SELECT * FROM {catalog_name}.optimized.optimized_table\n" \
                   f"WHERE id >= 1\n" \
                   f"ORDER BY created_at DESC\n" \
                   f"LIMIT 1000;"
        
        # –ü—ã—Ç–∞–µ–º—Å—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        optimized = original_query
        
        # –ó–∞–º–µ–Ω—è–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–∞—Ä—ã–µ —Ç–∞–±–ª–∏—Ü—ã –Ω–∞ –Ω–æ–≤—ã–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ
        import re
        
        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–∞–±–ª–∏—Ü—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ catalog.schema.table
        table_pattern = r'(\w+)\.(\w+)\.(\w+)'
        matches = re.findall(table_pattern, optimized)
        
        for catalog_part, schema_part, table_part in matches:
            if schema_part.lower() != 'optimized':
                old_reference = f"{catalog_part}.{schema_part}.{table_part}"
                new_reference = f"{catalog_name}.optimized.{table_part}"
                optimized = optimized.replace(old_reference, new_reference)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        if "ORDER BY" not in optimized.upper():
            optimized += "\nORDER BY 1 DESC LIMIT 1000;"
        elif "LIMIT" not in optimized.upper():
            optimized += " LIMIT 1000;"
        
        return f"-- Optimized version of original query\n{optimized}"
    
    def _extract_catalog_from_url(self, url: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–∏ –∫–∞—Ç–∞–ª–æ–≥–∞ –∏–∑ JDBC URL"""
        import re
        try:
            # –î–ª—è Trino URL: jdbc:trino://host:port?catalog=mycatalog
            trino_match = re.search(r'catalog=([^&]+)', url)
            if trino_match:
                catalog = trino_match.group(1)
                logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω –∫–∞—Ç–∞–ª–æ–≥ –∏–∑ Trino URL: {catalog}")
                return catalog
            
            # –î–ª—è –æ–±—ã—á–Ω—ã—Ö JDBC URL: jdbc://host:port/database
            if 'jdbc://' in url:
                url_part = url.replace('jdbc://', '')
                if '/' in url_part:
                    db_part = url_part.split('/')[-1]
                    if '?' in db_part:
                        db_name = db_part.split('?')[0]
                    else:
                        db_name = db_part
                    if db_name:
                        logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω –∫–∞—Ç–∞–ª–æ–≥ –∏–∑ JDBC URL: {db_name}")
                        return db_name
            
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–∞—Ç–∞–ª–æ–≥ –∏–∑ URL: {url}")
            return "default_catalog"
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–∞—Ç–∞–ª–æ–≥–∞ –∏–∑ URL {url}: {e}")
            return "default_catalog"
